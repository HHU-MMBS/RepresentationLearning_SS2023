{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "e-uirVvNW-yY"
   },
   "source": [
    "HHU Deep Learning, SS2022/23, 05.05.2023, Prof. Dr. Markus Kollmann\n",
    "\n",
    "Lecturers and Tutoring is done by Tim Kaiser, Nikolas Adaloglou and Felix Michels.\n",
    "\n",
    "# Assignment 05 - Contrastive self-supervised learning: SimCLR in STL10 with Resnet18 \n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Preparation and imports\n",
    "2. Implement the augmentation pipeline used in SimCLR\n",
    "3. Implement the SimCLR Contrastive loss (NT-Xent)\n",
    "4. Load and modify resnet18\n",
    "5. Gradient Accumulation: Implement the `training_step`  and `pretrain_one_epoch_grad_acc`\n",
    "6. Putting everything together and train the model\n",
    "7. Linear probing + T-SNE visualization of features\n",
    "8. Compare SimCLR versus supervised Imagenet-pretrained weights and random init on STL10 train/val split\n",
    "9. Plot the val accuracies for the 3 different initializations"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Contrastive loss is a way of training a machine learning model in a self-supervised manner, where the goal is to learn meaningful representations of the input data without any explicit labels or annotations.\n",
    "\n",
    "The basic idea is to take a pair of input samples (such as two augmented views from the same image), and compare them to see if they are similar or dissimilar. The model is then trained to push similar pairs closer together in the representation space, while pushing dissimilar pairs farther apart.\n",
    "\n",
    "To do this, the contrastive loss function measures the similarity between the representations of the two input samples (nominator), and encourages the model to maximize this similarity if the samples are similar, and minimize it if they are dissimilar.\n",
    "\n",
    "\n",
    "You can also advice the [SimCLR Paper](https://arxiv.org/abs/2002.05709)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "lw5K7r5SQDca"
   },
   "source": [
    "# Part I. Preparation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SIad0uuHBlv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import tqdm\n",
    "\n",
    "# Local imports\n",
    "from utils import *"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Implement the augmentation pipeline used in SimCLR\n",
    "\n",
    "In contrastive self-supervised learning, there are several image augmentations that are commonly used to create pairs of images that are transformed versions of each other. These augmentations are designed to ensure that the resulting views have enough differences between them so that the model can learn to distinguish between them, while also preserving the label-related information.\n",
    "\n",
    "Implement the following transformations **presented in random order**:\n",
    "\n",
    "\n",
    "- Random flipping: This involves randomly flipping the image horizontally or vertically. Choose the one that best fits with a probability of 50%.\n",
    "- Normalize the images with an appropriate mean std.\n",
    "- Color jitter: This involves randomly changing the brightness, contrast, saturation and hue (20%) of the image. This augmentation helps the model learn to recognize objects or scenes under different lighting conditions. Apply this augmentation with a probability of 80%. Distort the brightness, contrast, saturation in the range `[0.2, 1.8]`.\n",
    "- Random cropping: This involves randomly cropping a portion of the image to create a new image. We will then resize the images to 64x64 instead of 96x96 to reduce the computational time complexity to train the model.  Use a scale of 10-100% of the initial image size. \n",
    "- Gaussian blur: This augmentation helps the model learn to recognize objects or scenes that are slightly out of focus. Use a `kernel_size` of 3 and Standard deviation of 0.1 to 2.0.\n",
    "\n",
    "\n",
    "The above augmentations are typically applied randomly to each image in a pair, resulting in two slightly different versions of the same image that can be used for contrastive learning.\n",
    "\n",
    "Your task is to define the augmentation and decide in which order they should be applied. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Augment:\n",
    "    \"\"\"\n",
    "    A stochastic data augmentation module\n",
    "    Transforms any given data example randomly\n",
    "    resulting in two correlated views of the same example,\n",
    "    denoted x ̃i and x ̃j, which we consider as a positive pair.\n",
    "    \"\"\"\n",
    "    def __init__(self, img_size):\n",
    "        ### START CODE HERE ### (≈ 5 lines of code)\n",
    "        \n",
    "    def __call__(self, x):\n",
    "        # This function applied the same transformation to an image twice.\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "def load_data( batch_size=128, train_split=\"unlabeled\", test_split=\"test\", transf = T.ToTensor()):\n",
    "    # Returns a train and validation dataloader for STL10 dataset\n",
    "    ### START CODE HERE ### (≈ 6 lines of code)\n",
    "\n",
    "    ### END CODE HERE ###\n",
    "    return train_dl, val_dl"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4v-Qg5Xpk2Bv"
   },
   "source": [
    "# Part III. Implement the SimCLR Contrastive loss (NT-Xent)\n",
    "\n",
    "Let $sim(u,v)$ note the dot product between 2 normalized $u$ and $v$ (i.e. cosine similarity). Then the loss function for a **positive pair**\n",
    "of examples (i,j) is defined as:\n",
    "$$\n",
    "\\ell_{i, j}=-\\log \\frac{\\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_{i}, \\boldsymbol{z}_{j}\\right) / \\tau\\right)}{\\sum_{k=1}^{2 N} \\mathbb{1}_{[k \\neq i]} \\exp \\left(\\operatorname{sim}\\left(\\boldsymbol{z}_{i}, \\boldsymbol{z}_{k}\\right) / \\tau\\right)}\n",
    "$$\n",
    "\n",
    "where $\\mathbb{1}_{[k \\neq i]} $ ∈{0,1} is an indicator function evaluating to 1 iff $k != i$ and τ denotes a temperature parameter. The final loss is computed by summing all positive pairs and divide by $2\\times N = views \\times batch_{size} $\n",
    "\n",
    "There are different ways to develop contrastive loss. \n",
    "\n",
    "\n",
    "#### Hints\n",
    "Here we provide you with some hints about the main algorithm:\n",
    "\n",
    "- apply l2 normalization to the features and concatenate them in the batch dimension\n",
    "\n",
    "- Calculate the similarity/logits of all pairs.  Output shape:[batch_size $\\times$ views,batch_size $\\times$ views]\n",
    "\n",
    "- Make Identity matrix as mask with size=(batch_size $\\times$ views, batch_size $\\times$ views)\n",
    "\n",
    "- Repeat the mask in both direction to the number of views (in simclr number of views = 2)\n",
    "for batch_size=5 and 2 views: \n",
    "```\n",
    "[1., 0., 0., 0., 0., 1., 0., 0., 0., 0.]\n",
    "[0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "[0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
    "[0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
    "[0., 0., 0., 0., 1., 0., 0., 0., 0., 1.],\n",
    "[1., 0., 0., 0., 0., 1., 0., 0., 0., 0.],\n",
    "[0., 1., 0., 0., 0., 0., 1., 0., 0., 0.],\n",
    "[0., 0., 1., 0., 0., 0., 0., 1., 0., 0.],\n",
    "[0., 0., 0., 1., 0., 0., 0., 0., 1., 0.],\n",
    "[0., 0., 0., 0., 1., 0., 0., 0., 0., 1.]\n",
    "```\n",
    "\n",
    "4. Make a mask to index the positive pairs. mask-out the self-contrast as follows.\n",
    "make a mask with the shape of the logits = [batch_size $\\times$ views,batch_size $\\times$ views]  that has ones in the diagonals that are +- batch_size from the main diagonal. this will be used to index the positive pairs.\n",
    "Example for [6,6] matrix (batch_size=3,views=2):\n",
    "```\n",
    "[0., 0., 0., 1., 0., 0.],\n",
    "[0., 0., 0., 0., 1., 0.],\n",
    "[0., 0., 0., 0., 0., 1.],\n",
    "[1., 0., 0., 0., 0., 0.],\n",
    "[0., 1., 0., 0., 0., 0.],\n",
    "[0., 0., 1., 0., 0., 0.]\n",
    "``` \n",
    "Ones here will be the positive elements for the nominator.\n",
    "Alternativly you can use torch.diag() to take the positives from the  [6,6] similarity matrix (aka logits)\n",
    "\n",
    "- Use the positives to form the nominator.Scale down result with the temperature. There are batch_size $\\times$ views positive pairs.\n",
    "\n",
    "- Calculate the denominator by summing the masked logits in the correct dimension.\n",
    "\n",
    "- dont forget to apply `-log(result)`\n",
    "\n",
    "- Calculate the final loss as in the above equation.\n",
    "\n",
    "\n",
    "#### A note on L2 normalization\n",
    "\n",
    "L2 normalization is a common technique used in contrastive learning to normalize the embedding vectors before computing the contrastive loss. \n",
    "\n",
    "This is because L2 normalization scales the vectors to have unit length. Without L2 normalization, the magnitude of the embedding vectors can have a large influence on the contrastive loss. \n",
    "\n",
    "This can result in the optimization process focusing more on adjusting the magnitude of the vectors rather than their direction, leading to suboptimal solutions. \n",
    "\n",
    "By normalizing the embeddings, the contrastive loss only considers the angular difference between embedding vectors.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeP4ZuZpsyOp"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ContrastiveLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Vanilla Contrastive loss, also called InfoNceLoss as in SimCLR paper\n",
    "    There are different ways to develop contrastive loss. Here we provide you with some hints about the main algorithm:\n",
    "        1- create an Identity matrix as a mask (bsz, bsz)\n",
    "        2- repeat the mask in both direction to the number of views (in simclr number of views = 2) in the above code we called it anchor_count\n",
    "        3- modify the mask to remove the self contrast cases\n",
    "        4- calculate the similarity of two features. *Note: final size should be  [bsz, bsz]\n",
    "        5- apply the mask on similairty matrix \n",
    "        6- calculate the final loss \n",
    "    \"\"\"\n",
    "    ### START CODE HERE ###  (≈ 19 lines of code)\n",
    "    \n",
    "    def forward(self, proj_1, proj_2):\n",
    "        \"\"\"\n",
    "        proj_1 and proj_2 are batched embeddings [batch, embedding_dim]\n",
    "        where corresponding indices are pairs\n",
    "        z_i, z_j in the SimCLR paper\n",
    "        \"\"\"\n",
    "\n",
    "        return loss # scalar!\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "def test_ContrastiveLoss():\n",
    "    batch_size = 8\n",
    "    temperature = 0.1\n",
    "    criterion = ContrastiveLoss(batch_size, temperature)\n",
    "    proj_1 = torch.rand(batch_size, 128)\n",
    "    proj_2 = torch.rand(batch_size, 128)\n",
    "    loss = criterion(proj_1, proj_2)\n",
    "    assert loss.shape == torch.Size([]), \"ContrastiveLoss output shape is wrong\"\n",
    "    assert loss.item() >= 0, \"ContrastiveLoss output is negative\"\n",
    "    print(\"ContrastiveLoss test passed!\")\n",
    "\n",
    "test_ContrastiveLoss()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "F8iM6b8CQjSy"
   },
   "source": [
    "# Part IV. Load and modify resnet18\n",
    "\n",
    "- Load and modify the resnet18.\n",
    "- Add an MLP with batch normalization after the resnet18 backbone as illustrate below:\n",
    "```python\n",
    "Sequential(\n",
    "  (0): Linear(in_features=in_features, out_features=in_features, bias=False)\n",
    "  (1): BatchNorm(in_features)\n",
    "  (2): ReLU()\n",
    "  (3): Linear(in_features=in_features, out_features=embedding_size, bias=False)\n",
    "  (4): BatchNorm(embedding_size))\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WpEEBp7EH7-x"
   },
   "outputs": [],
   "source": [
    "class ResNetSimCLR(nn.Module):\n",
    "    def __init__(self, embedding_size=128):\n",
    "        super(ResNetSimCLR, self).__init__()\n",
    "        ### START CODE HERE ### (≈ 10 lines of code)\n",
    "        # load resnet18 pretrained on imagenet\n",
    "        # self.backbone = ...\n",
    "        # add mlp projection head\n",
    "        # self.projection = ....\n",
    "\n",
    "    def forward(self, x, return_embedding=False):\n",
    "\n",
    "    ### END CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "ppxywhSH_Xjc"
   },
   "source": [
    "# Part V. Implement the `training_step`  and `pretrain_one_epoch_grad_acc`\n",
    "\n",
    "### Gradient accumulation and mixed precision\n",
    "\n",
    "- `training_step` should load a batch of 2 image views and feed them to the model. The loss function will calculate the implemented SimCLR loss.\n",
    "- Gradient accumulation saves the gradient values for $N$ steps. It calculates the gradients and proceeds to the next batch. Remember that when you call `loss.backward()` the newly computed gradients are added to the old ones. After N steps, the parameter update is done and the loss shall be scaled down (averaged) by the number of N iterations.\n",
    "\n",
    "Note: SimCLR training requires a large batch size. You should be to train SimCLR with a batch size of at least 256 on Google Colab.\n",
    "\n",
    "#### Explanation of accumulated gradients\n",
    "\n",
    "When training large neural networks, the computational cost of computing the gradient for all of the training examples in the dataset can be prohibitive. Gradient accumulation is a technique used to increase the size of the batch of training samples used to update the weights of the network. \n",
    "\n",
    "Instead of applying the gradients to the model's parameters after each batch, the gradients are accumulated over a batch of training examples. The accumulated gradients are then used to update the model's parameters. In this way, one reduces the noise in the gradients by averaging them over a batch of training examples, which can lead to more stable updates to the model's parameters. It also allows the model to make larger updates to its parameters, which may speed up the training process.\n",
    "\n",
    "For example, if we set the batch size to 32, the network would process 32 examples at a time, compute the gradients for each example, and then accumulate the gradients over the 32 examples. After accumulating the gradients for the entire batch, the weights of the network are updated using the average of the accumulated gradients. Thus, for a batch size of 32 you can accumulate gradients every N steps so that you have an effective batch size of 32 $\\times$ N!\n",
    "\n",
    "> Importantly, gradient accumulation slows down training since gradient updates happen every N steps, but it is expected to see the loss dropping steadily and probably faster, depending on the method.\n",
    "\n",
    "### Mixed Precision\n",
    "\n",
    "At this point, we are introducing another technique to optimize GPU  memory usage to use larger batch sizes, mixed precision. The idea is to perform as many operations as possible in fp16, instead of the standard fp32, during training. This is not as simple as casting everything to fp16 however, because some operations are sensitive to underflow (being rounded to 0), especially the gradient itself. \n",
    "\n",
    "Luckily, there is a torch package for this, `torch.cuda.amp`. Feel free to check out the docs [here](https://pytorch.org/docs/stable/amp.html#) and some examples [here](https://pytorch.org/docs/stable/notes/amp_examples.html#amp-examples). This package takes care of the intricate things and you can go ahead and train. \n",
    "\n",
    "We are using two functions from the package here, `autocast` and `GradScaler`. Autocast is taking care of casting the correct tensors to fp16 and leaving the others unchanged. The GradScaler then makes sure that the gradients in the backward pass avoid numerical instabilities. \n",
    "\n",
    "Feel free to use this technique in future exercises to save some memory and speed up your training. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a5ukADmI_d_H"
   },
   "outputs": [],
   "source": [
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "def training_step(model, loss_function, data):\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "    return loss\n",
    "\n",
    "def pretrain_one_epoch_grad_acc(model, loss_function, train_dataloader, \n",
    "                                    optimizer, device, accum_iter=1, amp=False):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    num_batches = len(train_dataloader)\n",
    "    optimizer.zero_grad()\n",
    "    scaler = GradScaler() if amp else None\n",
    "    for batch_idx,data in enumerate(train_dataloader):\n",
    "        ### START CODE HERE ### ( > 6 lines of code)\n",
    "        if amp:\n",
    "            # ....\n",
    "        else:\n",
    "            #.......\n",
    "        \n",
    "        # weights update\n",
    "\n",
    "        # scale back the loss\n",
    "        # total_loss = ....\n",
    "\n",
    "        ### END CODE HERE ###\n",
    "    return total_loss/num_batches\n",
    "    \n",
    "\n",
    "\n",
    "def pretrain(model, optimizer, num_epochs, train_loader, criterion, device, accum_iter=1, amp=False):\n",
    "    dict_log = {\"train_loss\":[]}\n",
    "    best_loss = 1e8\n",
    "    model = model.to(device)\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        train_loss = pretrain_one_epoch_grad_acc(model, criterion, train_loader, optimizer,\n",
    "                                                    device, accum_iter, amp=amp)\n",
    "        msg = (f'Ep {epoch}/{num_epochs}: || Loss: Train {train_loss:.3f}')\n",
    "        pbar.set_description(msg)\n",
    "        dict_log[\"train_loss\"].append(train_loss)\n",
    "        \n",
    "        # Use this code to save the model with the lowest loss\n",
    "        if train_loss < best_loss:\n",
    "            best_val_loss = train_loss\n",
    "            save_model(model, f'best_model_min_train_loss.pth', epoch, optimizer, train_loss)   \n",
    "        if epoch == num_epochs - 1:\n",
    "            save_model(model, f'last_model_ep{epoch}.pth', epoch, optimizer, train_loss)\n",
    "    return dict_log"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "LqNuy5R2AThH"
   },
   "source": [
    "# Part VI. Putting everything together and train the model\n",
    "\n",
    "Hint: ~50 epochs should be sufficient to see the learned features.\n",
    "\n",
    "A small training trick here. We will exclude batch normalization parameters from weight decay in `define_param_groups`\n",
    "\n",
    "Note on complexity: 10.7 VRAM used and ~156mins needed. Effective batch size>1024, images of 64x64, 60 epochs.\n",
    "\n",
    "In case you face problem with Google colab, download the model every 5 epochs or better mount you google drive and save the model there in case you disconnect.\n",
    "\n",
    "Here\n",
    "```python\n",
    "PATH = './best_model.ckpt'\n",
    "torch.save(model_simclr.state_dict(), PATH)\n",
    "files.download(PATH)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hparams:\n",
    "    def __init__(self):\n",
    "        # This is what we used, feel free to change those parameters.\n",
    "        # You only need to specify the temperature in the config object\n",
    "        self.seed = 77777 # randomness seed\n",
    "        self.device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.img_size = 64 #image shape\n",
    "        self.load = False # load pretrained checkpoint\n",
    "        self.batch_size = 512\n",
    "        self.lr = 3e-4 # for ADAm only\n",
    "        self.weight_decay = 1e-6\n",
    "        self.embedding_size = 128 # papers value is 128\n",
    "       \n",
    "        self.epochs = 100\n",
    "        self.accum_iter = 1 # gradient accumulation\n",
    "        self.amp = True # automatic mixed precision\n",
    "        ############################################\n",
    "        # START CODE HERE ### (≈ 1 line of code)\n",
    "        self.temperature = ........\n",
    "        ### END CODE HERE ###\n",
    "\n",
    "### START CODE HERE ### (>10 lines of code)\n",
    "\n",
    "\n",
    "# Launch training i.e :\n",
    "# dict_log = pretrain(model, optimizer, config.epochs,\n",
    "#                     train_dl, criterion, \n",
    "#                     config.device, accum_iter=config.accum_iter,\n",
    "#                     amp=config.amp)\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Linear probing + T-SNE visualization of features\n",
    "\n",
    "As in the previous exercise, check the results of linear probing on the supervised training split and the T-SNE visualization.\n",
    "\n",
    "Code for the T-SNE visualization exists in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (> 10 lines of code)\n",
    "# model = ResNetSimCLR(embedding_size=config.embedding_size)\n",
    "# model = load_model(model, \"simclr.pth\")\n",
    "\n",
    "\n",
    "# Linear evaluation\n",
    "\n",
    "\n",
    "# TSNE plot\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "```\n",
    "Model simclr.pth is loaded from epoch 99 , loss 5.342101926069994\n",
    "Ep 199/200: Accuracy : Train:87.80 \t Val:78.41 || Loss: Train 0.360 \t Val 0.612\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "4FrwRzDnAst5"
   },
   "source": [
    "# Part VIII. Compare SimCLR versus supervised Imagenet-pretrained weights and random init on STL10 train/val split\n",
    "\n",
    "- Don't forget to use the train split of STL10 for supervised training.\n",
    "- For simplicity, don't use augmentations here, although it's possible and it would lead to better results.\n",
    "- Since we are not using any augmentations at this step, simclr will have the same results as before.\n",
    "\n",
    "\n",
    "Variants to be tested: \n",
    "- SimCLR weights trained for at least 50 epochs\n",
    "- Imagenet initialization\n",
    "- random initialization\n",
    "Afterward, print the best val. accuracy for all 3 models!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(mode='simclr'):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    ### START CODE HERE ### (≈ 15 lines of code)\n",
    "        \n",
    "    if mode == 'random':\n",
    "\n",
    "    elif mode == 'imagenet':\n",
    "\n",
    "    elif mode == 'simclr':\n",
    "        \n",
    "    ### END CODE HERE ###\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    dict_log = linear_eval(model, optimizer, 20, train_dl, val_dl, device)\n",
    "    return dict_log\n",
    "    \n",
    "\n",
    "dict_log_simclr = main('simclr')\n",
    "acc1 = np.max(dict_log_simclr[\"val_acc_epoch\"])\n",
    "dict_log_in = main('imagenet')\n",
    "acc2 = np.max(dict_log_in[\"val_acc_epoch\"])\n",
    "dict_log_ran = main('random')\n",
    "acc3 = np.max(dict_log_ran[\"val_acc_epoch\"])\n",
    "print(f\"Fine-tuning best results: SimCLR: {acc1:.2f}%, ImageNet: {acc2:.2f} %, Random: {acc3:.2f} %\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HPqA2qOp9vl6"
   },
   "source": [
    "### Expected results\n",
    "\n",
    "By fine-tuning all variants for 20 epochs this is what we got: \n",
    "\n",
    "```\n",
    "Fine-tuning best results: SimCLR: 77.26%, ImageNet: 76.25 %, Random: 53.83 %\n",
    "\n",
    "```"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IX. Plot the val accuracies for the 3 different initializations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(dict_log_simclr[\"val_acc_epoch\"], label=\"SimCLR\")\n",
    "plt.plot(dict_log_in[\"val_acc_epoch\"], label=\"ImageNet\")\n",
    "plt.plot(dict_log_ran[\"val_acc_epoch\"], label=\"Random\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Fine tuning results on STL-10\")\n",
    "plt.savefig(\"fine_tuning_results_stl10.png\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Bonus reads\n",
    "\n",
    "That's the end of this exercise. If you reached this point, congratulations!\n",
    "\n",
    "\n",
    "### Optional stuff\n",
    "\n",
    "- Improve SimCLR. Add the [LARS optimizer](https://gist.github.com/black0017/3766fc7c62bdd274df664f8ec03715a2) with linear warm + [cosine scheduler](https://pytorch.org/docs/stable/generated/torch.optim.lr_scheduler.CosineAnnealingLR.html?highlight=cosine%20scheduler#torch.optim.lr_scheduler.CosineAnnealingLR) + train for 200 epochs. Then make a new comparison!\n",
    "- Train on CIFAR100 and compare rotation prediction VS SimCLR pretraining on both datasets. Which pretext task is likely to work better there?"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "[Exercise 4] - SimCLR Resnet18 Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
