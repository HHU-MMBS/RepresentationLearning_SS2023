{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HHU Deep Learning, SS2022/23, 10.04.2023, Prof. Dr. Markus Kollmann\n",
    "\n",
    "Lecturers and Tutoring is done by Tim Kaiser, Nikolas Adaloglou and Felix Michels.\n",
    "\n",
    "# Assignment 04 - Language Representations using BERT\n",
    "\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Preparation and imports\n",
    "2. Data Preparation\n",
    "3. The pair dataset\n",
    "4. Masking\n",
    "5. The model\n",
    "6. Training\n",
    "7. Validation\n",
    "8. Conclusion and bonus read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "\n",
    "In this exercie you will train a small [BERT](https://arxiv.org/abs/1810.04805) model on the IMDB dataset (https://huggingface.co/datasets/imdb).\n",
    "You will then use the model to classify the sentiment of movie reviews and the sentiment of sentences from the Stanford Sentiment Treebank (SST2, https://huggingface.co/datasets/sst2)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Preparation and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# You may need to install some additional packages\n",
    "!pip install datasets transformers nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic imports\n",
    "import torch\n",
    "import numpy as np\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn import functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "# local imports\n",
    "from utils import load_imdb_dataset, load_sst2_dataset\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Tokenizer\n",
    "To use text in a neural network, we need to convert it to numbers. This is done by a tokenizer. The tokenizer splits the text into tokens and maps each token to an integer. The tokenizer also adds special tokens to the beginning and end of the sentence. These special tokens are used to indicate the beginning and end of the sentence and to separate two sentences.\n",
    "You won't need to use a tokenizer directly, but you should go through this section to get a better understanding of how it works.\n",
    "We'll use a variant of WordPiece tokenization, which is used by BERT. The tokenizer is implemented in the `transformers` library. For more information, see [here](https://huggingface.co/course/chapter6/6?fw=pt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import TOKENIZER"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common words are just tokens themselves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER.tokenize('These are just common words')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Less common words are split into subwords. This way we can represent rare words with a few tokens and still have a fixed vocabulary size.\n",
    "We also don't have to worry about encountering new words that are not in the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOKENIZER.tokenize('Gerbils are crepuscular')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special tokens and turn to ids\n",
    "Special tokens are indicated by brackets `[]`. These are the relevant special tokens:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Token  | Purpose                                              |\n",
    "|--------|------------------------------------------------------|\n",
    "| [CLS]  | The first token is always classification             |\n",
    "| [SEP]  | At the end of each sentence                          |\n",
    "| [PAD]  | Use to pad sentences to the same length              |\n",
    "| [MASK] | Used to create a mask by replacing the original word |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the tokenizer on a list of sentence pairs, it will automatically add the special tokens and separate the sentences as well as pad the sentences to the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_pairs = [\n",
    "    ['hello world', 'what a nice day'],\n",
    "    ['this is a slightly longer sentence', 'it really is'],\n",
    "    ['And this is a third sentence', 'just for good measure']\n",
    "]\n",
    "\n",
    "first_sentences, second_sentences = zip(*sentence_pairs)\n",
    "\n",
    "out = TOKENIZER(first_sentences, second_sentences, padding=True, return_tensors='pt', truncation=True, return_special_tokens_mask=True, return_attention_mask=False)\n",
    "input_ids = out['input_ids']\n",
    "segment_ids = out['token_type_ids']\n",
    "special_tokens_mask = out['special_tokens_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The input ids are just numbers, one number for each token.\n",
    "input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The tokenizer assigns ids between 0 and {TOKENIZER.vocab_size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can decode the input ids to get the original text together with the special tokens\n",
    "TOKENIZER.decode(input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The segment ids indicate which sentence each token belongs to. 0 is the first sentence, 1 is the second sentence.\n",
    "# Note that we could get this information just by looking at the [SEP] tokens\n",
    "segment_ids[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the first sentence\n",
    "TOKENIZER.decode(input_ids[0][segment_ids[0] == 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get just the second sentence\n",
    "TOKENIZER.decode(input_ids[0][segment_ids[0] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The special tokens mask indicates which tokens are special tokens\n",
    "# Just the special tokens\n",
    "TOKENIZER.decode(input_ids[0][special_tokens_mask[0] == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only the normal tokens\n",
    "TOKENIZER.decode(input_ids[0][special_tokens_mask[0] == 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "- Each entry in the IMDb dataset `dataset = load_imdb_dataset()` corresponds to a movie review consisting of a list of sentences. Since they are tokenized, each entry is a list of tensors containing integers.\n",
    "\n",
    "- We can use the `decode` method of the tokenizer to get the original sentences from the tokens.\n",
    "\n",
    "- Performance-related info: For performance reasons, we will use precomputed tokens. This means that we will not use the tokenizer directly, but instead use the precomputed tokens that are stored in the dataset. When you load the dataset for the first time, it will download the dataset and tokenize it. This may take 2-5 minutes. The tokens will be saved in a cache directory, so the next time you load the dataset, it will be faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_imdb_dataset()\n",
    "print('Number of training samples corresponding to movie reviews:', len(dataset))\n",
    "print(\"Each data instance (training sample) in the dataset is a list of tokenized sentences with len\", len(dataset[0]))\n",
    "\n",
    "print(\"The tokenized sentences are lists of token ids:\")\n",
    "for c, tokens in enumerate(dataset[0]):\n",
    "    print(c, tokens)\n",
    "    if c == 4:\n",
    "        break\n",
    "\n",
    "print(\"\\n The corresponding decoded text for the same training sample is:\")\n",
    "for c, tokens in enumerate(dataset[0]):\n",
    "    print(c, TOKENIZER.decode(tokens), \"\\n\")\n",
    "    if c == 4:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text only contains lower case letters and most punctuation has been removed. \n",
    "\n",
    "This helps because you will use a relatively small model and a small dataset.\n",
    "\n",
    "Since we use precomputed tokens, we cannot use the tokenizer to add special tokens. Instead, we will use the `add_special_tokens` function from the `utils` module.\n",
    "This function takes two tokenized sentences, adds the special tokens and concatenates them.\n",
    "It returns a dictionary with the keys:\n",
    "- `input_ids` and `segment_ids` like in the previous section.\n",
    "- The key `normal_mask` is the inverse of the `special_tokens_mask` from the previous section. It indicates which tokens are not special tokens and should only have three `True` entries: `[CLS]` at the beginning, `[SEP]` between the sentences and `[SEP]` at the end.\n",
    "\n",
    "Again, we can use the `decode` method of the tokenizer to get the original text from the tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import add_special_tokens\n",
    "out = add_special_tokens(dataset[0][0], dataset[5][6])\n",
    "print(out)\n",
    "print(TOKENIZER.decode(out['input_ids']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, all sequences will be limited by the hyper-parameter `MAX_SEQ_LEN` (again, for performance reasons)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MAX_SEQ_LEN\n",
    "print(f\"Maximum sequence length: {MAX_SEQ_LEN}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III: The pair dataset\n",
    "\n",
    "One of the pretext tasks in BERT is to train on next sentence prediction: whether 2 sentences come from the same text.\n",
    "\n",
    "For this we need a dataset that contains pairs of sentences, some of which are consecutive sentences from the same text and some of which are random sentences from different texts.\n",
    "\n",
    "### Task 1 - Implement the `IMDbPairDataset` class. \n",
    "\n",
    "Each item in this dataset are a random pair of tokenized sentences from the IMDb dataset. Because this dataset return random pairs, we will not use a standard `torch.utils.data.DataSet` class, but the subclass `torch.utils.data.IterableDataset` [link](https://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset).\n",
    "\n",
    "We also want to make sure that the sentences are not too short or too long. The `min_len` and `max_len` arguments specify the minimal and maximal length of the combined sentences.\n",
    "You can just sample inside a loop until you find a pair of sentences that satisfies the length constraints.\n",
    "If you want, try to find a more efficient way to do this.\n",
    "\n",
    "Recall, that each entry in the IMDb dataset is a list of tokenized sentences. Therefore, each sentence is described by a *pair* of indices: the index of the text (movie review) and the index of the sentence inside the text.\n",
    "\n",
    "Please read the method docstrings that give you further instructions.\n",
    "\n",
    "For sampling, we used `np.random.randint()` so you need to sample with the same function to get our expected result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IMDbPairDataset(torch.utils.data.IterableDataset):\n",
    "\n",
    "    def __init__(self, min_len=0, max_len=MAX_SEQ_LEN, p_next=0.5, train=True):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            min_len: minimal length of both combined sentences\n",
    "            max_len: maximal length of both combined sentences\n",
    "            p_next: probability that the second sentence is the next sentence in the text\n",
    "            train: whether to use the train or test dataset\n",
    "        \"\"\"\n",
    "        self.max_len = max_len\n",
    "        self.min_len = min_len\n",
    "        self.dataset = load_imdb_dataset(train)\n",
    "        self.p_next = p_next\n",
    "\n",
    "    def sample_next_idx(self):\n",
    "        \"\"\"\n",
    "        Sample two pair of indices corresponding to two consecutive sentences from the dataset, i.e.,\n",
    "        the second sentence is the next sentence in the same text as the first sentence.\n",
    "        Note, that some texts only have one sentence. In this case, you should sample again.\n",
    "\n",
    "        Returns:\n",
    "            two tuples of the form (text_idx, sent_idx), (text_idx, sent_idx + 1)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 5 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        return (text_idx, sent_idx), (text_idx, sent_idx + 1)\n",
    "\n",
    "    def sample_random_idx(self):\n",
    "        \"\"\"\n",
    "        Sample two pair of indices corresponding to two random sentences from the dataset.\n",
    "\n",
    "        Returns:\n",
    "            two tuples of the form (text_idx_a, sent_idx_a), (text_idx_b, sent_idx_b)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 6 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        return (text_idx_a, sent_idx_a), (text_idx_b, sent_idx_b)\n",
    "\n",
    "    def sample_tokens(self, is_next):\n",
    "        \"\"\"\n",
    "        Samples a pair of sentences from the dataset by first sampling indices and then using the `add_special_tokens` function.\n",
    "        Args:\n",
    "            is_next: if True, the second sentence is the next sentence in the same movie review.\n",
    "                     if False, the second sentence is a random sentence from a random movie review.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with the keys 'input_ids', 'segment_ids' and 'normal_mask'.\n",
    "            The values are tensors of the same length without length restriction.\n",
    "\n",
    "        \"\"\"\n",
    "        if is_next:\n",
    "            (text_idx_a, sent_idx_a), (text_idx_b, sent_idx_b) = self.sample_next_idx()\n",
    "        else:\n",
    "            (text_idx_a, sent_idx_a), (text_idx_b, sent_idx_b) = self.sample_random_idx()\n",
    "        tokens_a = self.dataset[text_idx_a][sent_idx_a]\n",
    "        tokens_b = self.dataset[text_idx_b][sent_idx_b]\n",
    "        return add_special_tokens(tokens_a, tokens_b)\n",
    "\n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Samples a random pair of sentences from the dataset.\n",
    "        With probability p_next, the second sentence is the next sentence in the same movie review.\n",
    "        With probability 1 - p_next, the second sentence is a random sentence from a random movie review.\n",
    "\n",
    "        Returns:\n",
    "            A dictionary with the keys 'input_ids', 'segment_ids', 'normal_mask'\n",
    "            and 'is_next' (boolean, indicates whether the second sentence is the next sentence in the same movie review).\n",
    "            The values are tensors of the same length, but at least min_len and at most max_len.\n",
    "        \"\"\"\n",
    "        is_next = np.random.rand() < self.p_next\n",
    "        out = self.sample_tokens(is_next)\n",
    "        while not (self.min_len <= len(out['input_ids']) <= self.max_len):\n",
    "            out = self.sample_tokens(is_next)\n",
    "        out.update(is_next=is_next)\n",
    "        return out\n",
    "\n",
    "    def __iter__(self):\n",
    "        while True:\n",
    "            yield self.sample()\n",
    "\n",
    "def test_IMDbPairDataset():\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    dataset = IMDbPairDataset()\n",
    "    for sample in dataset:\n",
    "        print(\"Each training sample contains the following keys:\", list(sample.keys()), \"\\n\")\n",
    "        print(\"Is next:\", sample['is_next'])\n",
    "        print(TOKENIZER.decode(sample['input_ids']))\n",
    "        break\n",
    "\n",
    "test_IMDbPairDataset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "Note: You might get a different result based on your implementation of the random samling.\n",
    "\n",
    "```\n",
    "Each training sample contains the following keys: ['input_ids', 'normal_mask', 'segment_ids', 'is_next'] \n",
    "\n",
    "Is next: False\n",
    "\n",
    "[CLS] actually i'd like to see a celebrity death match with rambo and braddock though it wouldn't be much of a fight and then there's good old chuck norris who for some reason a few people find entertaining but honestly the greatest entertainment he gives me is the chuck norris facts this film is pretty easy to laugh at though but i don't think it was meant to be funny [SEP] nick nolte and others involved with the movie vowed never to work with eddie murphy again [SEP]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV: Masking\n",
    "\n",
    "The other pretext task BERT is trained on is *masked language modeling*, in which some tokens are replaced by a special `[MASK]` token and the model is trained to predict the original token.\n",
    "\n",
    "### Task 2: Implement the `BERTMasker` class, which takes a sequence of tokens and masks some of them.\n",
    "\n",
    "\n",
    "\n",
    "The masking procedure is as follows:\n",
    "First, select the tokens to be randomly masked with probability `p_select`. \n",
    "\n",
    "`Note`: Only consider tokens that are \"normal\", i.e., not special tokens (i.e., not `[CLS]`, `[SEP]` or `[PAD]`). Use the `normal_mask` to achieve this.\n",
    "\n",
    "Second, for each selected masked token there are three possibilities: replace it with a random token (with prob. `p_rand`), mask it by using the token TOKENIZER.mask_token_id (with prob. p_mask), or leave it as is.\n",
    "\n",
    "In summary:\n",
    "- `p_mask`, replace the token with `TOKENIZER.mask_token_id` (The id corresponding to the `[MASK]` token)\n",
    "- `p_rand`, replace the token with a random token from the vocabulary, which is {0,..., `TOKENIZER.vocab_size`-1}\n",
    "- `1 - p_mask - p_rand`, keep the token as is.\n",
    "\n",
    "Implement the  `__call__(self, input_ids, normal_mask)` method that return the modified input tokens, the mask that indicates which tokens are alterted with probability `p_select` and the .\n",
    "\n",
    "\n",
    "`Hints` based on our implementation:\n",
    "\n",
    "Implementation-wise, the class member `self.p_mods = torch.tensor([p_mask, p_rand, 1 - p_mask - p_rand])` has entries in 0,1,2 which decide which option happens for each token. Here is an example:\n",
    "\n",
    "```python\n",
    "self.p_mods = torch.tensor([p_mask, p_rand, 1 - p_mask - p_rand])\n",
    "mod_mask = torch.multinomial(self.p_mods, input_ids.numel(), replacement=True)\n",
    "# mod_mask will have the values of [0,1,2]\n",
    "```\n",
    "You are FREE to implement this however you choose though!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTMasker:\n",
    "    def __init__(self, p_select=0.15, p_mask=0.8, p_rand=0.1):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: size of the vocabulary (number of tokens)\n",
    "            p_select: probability, that a token is selected for masking\n",
    "            p_mask: probability, that a selected token is replaced by [MASK]\n",
    "            p_rand: probability, that a selected token is replaced by a random token\n",
    "        \"\"\"\n",
    "        assert 0 <= p_select <= 1\n",
    "        assert 0 <= p_mask <= 1\n",
    "        assert 0 <= p_rand <= 1\n",
    "        assert p_mask + p_rand <= 1\n",
    "\n",
    "        self.p_select = p_select\n",
    "        # You can use p_mods together with torch.multinomial\n",
    "        self.p_mods = torch.tensor([p_mask, p_rand, 1 - p_mask - p_rand])\n",
    "\n",
    "    def __call__(self, input_ids, normal_mask):\n",
    "        \"\"\"\n",
    "        Masks the tokens\n",
    "        Args:\n",
    "            input_ids: a batch of sequences of tokens, a tensor of shape (batch_size, seq_len)\n",
    "            normal_mask: a boolean mask indicating which tokens are \"normal\", i.e., not special tokens, shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            input_ids: the modified input_ids, shape (batch_size, seq_len)\n",
    "            select_mask: a boolean mask indicating which tokens were selected for masking, shape (batch_size, seq_len)\n",
    "            masked_tokens: the original tokens that were selected (no matter wheter they are masked, replaced by random tokens, or not changed),\n",
    "                           shape (num_masked_tokens,), where num_masked_tokens ≈ batch_size * seq_len * p_select\n",
    "\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (5 lines of code)\n",
    "        # create a boolean mask indicating which tokens are selected for masking\n",
    "        \n",
    "        # apply the mask to input_ids\n",
    "        \n",
    "        # sample a modification for each selected token\n",
    "        \n",
    "        # apply the modification to the selected tokens\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        return input_ids, select_mask, masked_tokens\n",
    "\n",
    "def test_BERTMasker():\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    masker = BERTMasker()\n",
    "    input_ids = torch.tensor([[1, 2, 3, 4, 5, 6, 7, 8, 9],\n",
    "                                [1, 2, 3, 4, 5, 6, 7, 8, 9]])\n",
    "    normal_mask = torch.tensor([[False, True, True, True, True, True, True, True, False],\n",
    "                                    [False, True, True, True, True, True, True, True, False]])\n",
    "    input_ids, select_mask, masked_tokens = masker(input_ids, normal_mask)\n",
    "    print(\"input_ids:\", input_ids, input_ids.shape)\n",
    "    print(\"select_mask:\", select_mask, select_mask.shape)\n",
    "    print(\"masked_tokens:\", masked_tokens, masked_tokens.shape)\n",
    "\n",
    "test_BERTMasker()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "input_ids: tensor([[  1,   2, 103,   4,   5,   6,   7, 103,   9],\n",
    "        [  1, 103,   3,   4,   5,   6, 103,   8,   9]]) torch.Size([2, 9])\n",
    "select_mask: tensor([[False, False,  True, False, False, False, False,  True, False],\n",
    "        [False,  True, False, False, False, False,  True, False, False]]) torch.Size([2, 9])\n",
    "masked_tokens: tensor([3, 8, 2, 7]) torch.Size([4])\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. The BERT model\n",
    "\n",
    "Now you will implement the BERT model. The model consists of three parts:\n",
    "1. The embedding layer, which turns sequence of token ids into a sequence of embedding vectors.\n",
    "2. The transformer encoder, which is a stack of transformer encoder layers. The output of this model will be the representation used in the downstream tasks.\n",
    "3. The two classification heads, which take the output of the transformer encoder and predict the masked tokens and the next sentence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3: Implement the embedding layer.\n",
    "\n",
    "\n",
    "The embedding is the first layer in BERT that takes the input and creates a lookup table. The parameters of the embedding layers are learnable, which means when the learning process is over the embeddings will cluster similar words together. The embedding layer also preserves different relationships between words such as: semantic, syntactic, linear, and since BERT is bidirectional it will also preserve contextual relationships as well.\n",
    "\n",
    "- In the case of BERT, it creates three embeddings for Token, Segments and Position. Each of them can be implemented using `nn.Embedding`. \n",
    "- `Hint`: the final embedding is the sum of the token, position and segment embeddings and **layer normalization** is applied.\n",
    "- `Hint`: positional embeds may need to be broadcasted to the whole batch: (seq_len,) -> (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, d_model, max_len, n_segments=2):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            vocab_size: total number of tokens, used for the token embedding\n",
    "            d_model: embedding dimension, used for all three embeddings\n",
    "            max_len: maximum length of a sequence, used for the position embedding\n",
    "            n_segments: number of segments (for us always 2), used for the segment embedding\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ### START CODE HERE ### (4 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "\n",
    "    def forward(self, token_ids, segment_ids):\n",
    "        \"\"\"\n",
    "        Creates the combined embedding vectors for the input tokens and segments\n",
    "        Args:\n",
    "            token_ids: tensor of token ids, shape (batch_size, seq_len)\n",
    "            segment_ids: tensor of segment ids, shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            combined embeddings, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 6 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "def test_Embedding():\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    embedding = Embedding(vocab_size=100, d_model=10, max_len=512)\n",
    "    token_ids = torch.randint(100, size=(2, 10))\n",
    "    segment_ids = torch.randint(2, size=(2, 10))\n",
    "    print(embedding(token_ids, segment_ids).mean(), embedding(token_ids, segment_ids).std(), embedding(token_ids, segment_ids).sum())\n",
    "\n",
    "test_Embedding()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "tensor(-4.7684e-09, grad_fn=<MeanBackward0>) tensor(1.0025, grad_fn=<StdBackward0>) tensor(-9.5367e-07, grad_fn=<SumBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 4: Implement the BERT encoder.\n",
    "\n",
    "For the encoder you can use `nn.TransformerEncoder` combined with `nn.TransformerEncoder`.\n",
    "`nn.TransformerEncoderLayer` by default takes input of the shape `(seq_len, batch_size, d_model)`, but we recommend to have the batch size as the first dimension. Do do this, set `batch_first=True` in `nn.TransformerEncoderLayer`.\n",
    "\n",
    "\n",
    "Notes on the Transformer Encoder:\n",
    "- Because we set batch_first=True, the input shape is (batch_size, seq_len, d_model)\n",
    "- Use TOKENIZER.vocab_size and MAX_SEQ_LEN to initialize the embedding layer.\n",
    "\n",
    "Note, that that transformer encoder should get a mask to indicate where padding is (src_key_padding_mask), so that the padding tokens are not used to calculate the attention. We make the segment_ids optional, because some downstream tasks are only one sentence long. However, the embedding still needs the segment_ids, so set them to zeros if they are not passed.\n",
    "\n",
    "Here is an example (pad_mask need to be defined):\n",
    "```python\n",
    "self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTEncoder(nn.Module):\n",
    "    def __init__(self, d_model, n_layers, n_heads, dim_feedforward, dropout):\n",
    "        \"\"\"\n",
    "        Initialized both the transformer encoder and the embedding layer.\n",
    "        Args:\n",
    "            d_model: the dimension of the embedding vectors\n",
    "            n_layers: the number of transformer layers\n",
    "            n_heads: the number of heads in the multi-headed attention\n",
    "            dim_feedforward: the dimension of the feedforward network model\n",
    "            dropout: the dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.d_model = d_model\n",
    "        # self.embedding and encoder_layer need to be defined below\n",
    "        ### START CODE HERE ### (2 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=n_layers)\n",
    "\n",
    "    def forward(self, input_ids, segment_ids=None):\n",
    "        \"\"\"\n",
    "        Encodes a batch of sequences by\n",
    "        1. Embedding the tokens\n",
    "        2. Encoding the embedded tokens with the transformer encoder\n",
    "\n",
    "        Args:\n",
    "            input_ids: tensor of shape (batch_size, seq_len)\n",
    "            segment_ids: Optional, tensor of shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:the encoded representation of the input, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (4 lines of code)\n",
    "       \n",
    "        ### END CODE HERE ###\n",
    "        return self.encoder(x, src_key_padding_mask=pad_mask)\n",
    "\n",
    "def test_BERTEncoder():\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    encoder = BERTEncoder(d_model=10, n_layers=2, n_heads=2, dim_feedforward=20, dropout=0.1)\n",
    "    input_ids = torch.randint(100, size=(2, 10))\n",
    "    out = encoder(input_ids)\n",
    "    print(out.shape, out.mean(), encoder(input_ids).std(), encoder(input_ids).sum())\n",
    "\n",
    "test_BERTEncoder()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "torch.Size([2, 10, 10]) tensor(1.0729e-08, grad_fn=<MeanBackward0>) tensor(1.0025, grad_fn=<StdBackward0>) tensor(0., grad_fn=<SumBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 5: Implement the BERT model (encoder + the 2 task-specific heads)\n",
    "\n",
    "- Combine the encoder with the next sentence prediction and masked language model heads.\n",
    "- Both heads are just linear layers.\n",
    "- The next sentence classifier takes the the representation of the first token (the `[CLS]` token) and outputs a probability for the next sentence being the true next sentence.\n",
    "- The masked language model head takes the representation of all selected tokens and outputs a probability distribution over all tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERT(nn.Module):\n",
    "    def __init__(self, d_model, n_layers, n_heads, dim_feedforward, dropout):\n",
    "        \"\"\"\n",
    "        Initializes a BERTEncoder, a next sentence prediction head and a masked language model head.\n",
    "        Args:\n",
    "            d_model: the dimension of the embedding vectors\n",
    "            n_layers: the number of transformer layers\n",
    "            n_heads: the number of heads in the multi-headed attention\n",
    "            dim_feedforward: the dimension of the feedforward network model\n",
    "            dropout: the dropout rate\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        ### START CODE HERE ### (≈ 3 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "\n",
    "    def forward(self, input_ids, segment_ids, select_mask):\n",
    "        \"\"\"\n",
    "        Runs the forward pass of the BERT model.\n",
    "        Args:\n",
    "            input_ids: tensor of token ids, shape (batch_size, seq_len)\n",
    "            segment_ids: tensor of segment ids, shape (batch_size, seq_len)\n",
    "            select_mask: mask of tokens to predict, shape (batch_size, seq_len)\n",
    "\n",
    "        Returns:\n",
    "            logits_clsf: logits for next sentence prediction, shape (batch_size,)\n",
    "            logits_lm: logits for masked language model, shape (num_selected_tokens, vocab_size),\n",
    "            where num_selected_tokens ≈ batch_size * seq_len * p_select (see the masking section in the notebook)\n",
    "\n",
    "        \"\"\"\n",
    "        ### START CODE HERE ### (≈ 5 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "\n",
    "def test_BERT():\n",
    "    np.random.seed(999)\n",
    "    torch.manual_seed(999)\n",
    "    model = BERT(d_model=10, n_layers=2, n_heads=2, dim_feedforward=20, dropout=0.1)\n",
    "    input_ids = torch.randint(100, size=(2, 10))\n",
    "    segment_ids = torch.randint(2, size=(2, 10))\n",
    "    select_mask = torch.randint(2, size=(2, 10), dtype=torch.bool)\n",
    "    logits_clsf, logits_lm = model(input_ids, segment_ids, select_mask)\n",
    "    print(logits_clsf.shape, logits_lm.shape, logits_clsf.mean(), logits_lm.mean(), logits_clsf.std(), logits_lm.std(), logits_clsf.sum(), logits_lm.sum())\n",
    "\n",
    "test_BERT()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "```\n",
    "torch.Size([2]) torch.Size([9, 30522]) tensor(-0.6613, grad_fn=<MeanBackward0>) tensor(-0.0010, grad_fn=<MeanBackward0>) tensor(0.4044, grad_fn=<StdBackward0>) tensor(0.5771, grad_fn=<StdBackward0>) tensor(-1.3225, grad_fn=<SumBackward0>) tensor(-287.4517, grad_fn=<SumBackward0>)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI. Training\n",
    "\n",
    "Implement the training for one batch.\n",
    "The training should consist of the following steps:\n",
    "1. Transfer the batch (dict consisting of `input_ids`, `segment_ids`, `select_mask`, `is_next`) to the device\n",
    "2. Mask tokens in the batch\n",
    "3. Run the forward pass of the model\n",
    "4. Compute the loss, which is the sum of the cross entropy loss of the next sentence prediction and the masked language model\n",
    "5. Run the backward pass\n",
    "6. Compute the accuracy of the next sentence prediction (`clsf_acc`) and the masked language model (`lm_acc`) for logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import MovingAverage\n",
    "\n",
    "def train_bert(model, loader, masker, device, num_iterations=20000, warmup_steps=100, log_every=100, lr=1e-4):\n",
    "    model = model.to(device).train()\n",
    "    opt = torch.optim.AdamW(model.parameters(), lr=lr)\n",
    "    scheduler = torch.optim.lr_scheduler.LinearLR(opt, start_factor=1e-3, end_factor=1.0, total_iters=warmup_steps)\n",
    "    loss_avg = MovingAverage()\n",
    "    clsf_acc_avg = MovingAverage()\n",
    "    lm_acc_avg = MovingAverage()\n",
    "    hist = {'loss': [], 'clsf_acc': [], 'lm_acc': [], 'iter': []}\n",
    "    with tqdm(zip(range(num_iterations), loader), total=num_iterations) as pbar:\n",
    "        for it, batch in pbar:\n",
    "            opt.zero_grad(set_to_none=True)\n",
    "\n",
    "            ### START CODE HERE ### (≈ 15 lines of code)\n",
    "           \n",
    "            ### END CODE HERE ###\n",
    "            loss_avg.update(loss)\n",
    "            clsf_acc_avg.update(clsf_acc)\n",
    "            lm_acc_avg.update(lm_acc)\n",
    "\n",
    "            pbar.set_postfix(\n",
    "                loss=loss_avg.get(),\n",
    "                clsf_acc=clsf_acc_avg.get(),\n",
    "                lm_acc=lm_acc_avg.get()\n",
    "            )\n",
    "            if it % log_every == 0:\n",
    "                hist['iter'].append(it)\n",
    "                hist['loss'].append(loss_avg.get())\n",
    "                hist['clsf_acc'].append(clsf_acc_avg.get())\n",
    "                hist['lm_acc'].append(lm_acc_avg.get())\n",
    "\n",
    "            opt.step()\n",
    "            scheduler.step()\n",
    "    hist = {k: np.array(v) for k, v in hist.items()}\n",
    "    return hist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Validation\n",
    "\n",
    "Write the code to validate the model by computing the average `clsf_acc` and `lm_acc` over the validation set.\n",
    "\n",
    "Since both the masker and the dataset are not deterministic, we will just run the model on the validation set for a fixed number of iterations.        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import padded_collate\n",
    "\n",
    "@torch.no_grad()\n",
    "def validate_bert(model, masker, device, num_iterations=1000):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "        A dictionary with the keys 'clsf_acc' and 'lm_acc' containing the average accuracy\n",
    "        of the next sentence prediction and the masked language model respectively.\n",
    "    \"\"\"\n",
    "    dataset = IMDbPairDataset(train=False)\n",
    "    loader = DataLoader(dataset, batch_size=64, collate_fn=padded_collate, num_workers=4)\n",
    "    model.to(device).eval()\n",
    "    clsf_accs = []\n",
    "    lm_accs = []\n",
    "    for _, batch in tqdm(zip(range(num_iterations), loader), total=num_iterations):\n",
    "        ### START CODE HERE ### (≈ 6 lines of code)\n",
    "       \n",
    "        ### END CODE HERE ###\n",
    "        \n",
    "        clsf_accs.append(((logits_clsf > 0) == is_next).float().cpu())\n",
    "        lm_accs.append((logits_lm.argmax(dim=-1) == masked_tokens).float().cpu())\n",
    "    return dict(\n",
    "        clsf_acc=torch.cat(clsf_accs).mean().item() * 100,\n",
    "        lm_acc=torch.cat(lm_accs).mean().item() * 100)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII. Conclusion and Bonus reads\n",
    "\n",
    "That's the end of this exercise. If you reached this point, congratulations!\n",
    "\n",
    "- Coding-wise, that's the end of the currect exercise! Below is the rest of the code to study how to train and evaluate BERT.\n",
    "- Please run the rest of the cells if everything works up to know, so that we can see if you got the expected results!\n",
    "\n",
    "#### Hyperparameters to train the model\n",
    "The IMDb dataset contains 500000+ sentences. With a batch size of 64 (sentence pairs) we will therefore need at least need 4000 iterations to train the model for one epoch. We will train for 15000 iterations, which is a bit over 3 epochs.\n",
    "Feel free to train for more iterations or to change other hyperparameters such as the learning rate if you have the time and resources.\n",
    "The following hyperparameters work quite well though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import padded_collate\n",
    "\n",
    "# Model hyperparameters\n",
    "model_kwargs = dict(\n",
    "    d_model=512,\n",
    "    n_layers=8,\n",
    "    n_heads=8,\n",
    "    dim_feedforward=1024,\n",
    "    dropout=0.1\n",
    ")\n",
    "bert_model = BERT(**model_kwargs)\n",
    "\n",
    "# Training hyperparameters\n",
    "NUM_ITERATIONS = 15000\n",
    "LR=2e-4\n",
    "BATCH_SIZE=64\n",
    "\n",
    "masker = BERTMasker()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the training history. You will likely see a drop in the lm_acc but a rise in the clsf_acc when switching to longer sequences. Why do you think this is?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the first 90% of training, we use short sequences\n",
    "loader_short = DataLoader(IMDbPairDataset(max_len=128), batch_size=BATCH_SIZE, collate_fn=padded_collate, num_workers=4)\n",
    "hist1 = train_bert(bert_model, loader_short, masker,\n",
    "                   device=device, num_iterations=int(0.9 * NUM_ITERATIONS), warmup_steps=int(0.09 * NUM_ITERATIONS), log_every=100, lr=LR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the last 10% of training, we use a longer sequences. This is necessary to train the positional embeddings.\n",
    "# We divide the batch size by 2 to avoid running out of memory and also scale the learning rate by 0.5.\n",
    "loader_long = DataLoader(IMDbPairDataset(min_len=64, max_len=MAX_SEQ_LEN), batch_size=BATCH_SIZE // 2, collate_fn=padded_collate, num_workers=4)\n",
    "hist2 = train_bert(bert_model, loader_long, masker,\n",
    "                   device=device, num_iterations=int(0.1 * NUM_ITERATIONS), warmup_steps=0, log_every=100, lr=LR / 2)\n",
    "\n",
    "torch.save(bert_model.state_dict(), 'BERT.pt')\n",
    "torch.save(hist1, 'BERT_hist1.pt')\n",
    "torch.save(hist2, 'BERT_hist2.pt')\n",
    "\n",
    "# Combine the two training histories\n",
    "hist2['iter'] += hist1['iter'][-1]\n",
    "hist = {k: np.concatenate([v1, v2]) for k, v1, v2 in zip(hist1.keys(), hist1.values(), hist2.values())}\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(hist['iter'], hist['loss'], label='loss')\n",
    "plt.plot(hist['iter'], hist['clsf_acc'], label='clsf_acc')\n",
    "plt.plot(hist['iter'], hist['lm_acc'], label='lm_acc')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The next sentence prediction accuracy (clsf_acc) should be at least 60% and the masked language model accuracy (lm_acc) should be at least 30%. However, you can easily achieve 70% and 40% respectively by training for a longer time.\n",
    "bert_model.load_state_dict(torch.load('BERT.pt'))\n",
    "validate_bert(bert_model, masker, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "```\n",
    "{'clsf_acc': 63.76405954360962, 'lm_acc': 32.96291530132294}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating the model\n",
    "As the downstream task we will use the Stanford Sentiment Treebank (SST-2) dataset. This dataset contains 10,000 sentences with labels 0 (negative) and 1 (positive).\n",
    "\n",
    "`SST2Model` class: This class takes an instance of a `BERTEncoder` and initializes a linear classifier on top of the first token of the output of the encoder (like the next sentece classifier before). Optionally we want to be able to freeze the encoder and only train the classifier.\n",
    "You can achieve this, by setting `param.requires_grad = train_encoder` for each parameter of the encoder or using `torch.no_grad()` when calling the encoder.\n",
    "\n",
    "The dataset class, training and evaluation code is provided in utils.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the model on the SST-2 dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can try to train the encoder from scratch on the SST-2 dataset. However, even with more epochs you will have a hard time getting much more than 60% accuracy.\n",
    "\n",
    "When we use the encoder pretrained on IMDb, we can get 70% accuracy in just 3 epochs with linear probing. Note that in the original paper, the authors fine-tune the whole model for the downstream tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import train_sst2, validate_sst2\n",
    "\n",
    "model = BERT(**model_kwargs)\n",
    "sst2_model_scratch = train_sst2(model.encoder, train_encoder=True, device=device, epochs=5, lr=5e-4)\n",
    "val_acc_rand_init = validate_sst2(sst2_model_scratch, device=device)\n",
    "print(\"Val acc from random initialization:\", val_acc_rand_init)\n",
    "\n",
    "model = BERT(**model_kwargs)\n",
    "model.load_state_dict(torch.load('BERT.pt'))\n",
    "sst2_model = train_sst2(model.encoder, train_encoder=False, device=device, epochs=3)\n",
    "\n",
    "# You should get at least 70% accuracy on SST-2.\n",
    "val_acc = validate_sst2(sst2_model, device=device)\n",
    "print(\"Val accuracy:\", val_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "Val acc from random initialization: 58.830273151397705\n",
    "Val accuracy: 73.050457239151\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
