{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCfZVwhwMIYN"
   },
   "source": [
    "HHU Deep Learning, SS2022/23, 09.06.2023, Prof. Dr. Markus Kollmann\n",
    "\n",
    "Lecturers and Tutoring is done by Tim Kaiser, Nikolas Adaloglou and Felix Michels.\n",
    "\n",
    "# Assignment 10 - Contrastive Language-Image Pre-training for unsupervised out-of-distribution detection\n",
    "\n",
    "Copyright © 2023 Nikolas Adaloglou, Tim Kaiser and Felix Michels\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Basic imports\n",
    "2. Get the visual features of the CLIP model\n",
    "3. Compute the k-NN similarity as the OOD score\n",
    "4. Compute MSP using the text encoder and the label names\n",
    "5. Linear probing on the pseudolabels\n",
    "6. Mahalanobis distance as OOD score\n",
    "7. Mahalanobis distance using the real labels without linear probing\n",
    "8. K-means clusters combined with Mahalanobis distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Overview\n",
    "We will apply the learned representations from Contrastive Language-Image Pretrained (CLIP) on the downstream task of out-of-distribution detection.\n",
    "\n",
    "`Note`: I used the pretrained models from open_clip_torch, you can install it with `!pip install open_clip_torch`\n",
    "\n",
    "We will be using the model 'convnext_base_w' pretrained on 'laion2b_s13b_b82k' throughout this tutorial.\n",
    "\n",
    "Info and examples on how to use CLIP models for inference is provided in [openclip](https://github.com/mlfoundations/open_clip#usage)\n",
    "\n",
    "- [Learning Transferable Visual Models From Natural Language Supervision](https://arxiv.org/abs/2103.00020)\n",
    "- [Contrastive Language-Image Pretrained (CLIP) Models are Powerful Out-of-Distribution Detectors](https://arxiv.org/abs/2303.05828)\n",
    "\n",
    "\n",
    "\n",
    "# Part I. Basic imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "import torch\n",
    "import open_clip\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "from torch.utils.data import Subset, DataLoader, Dataset\n",
    "\n",
    "out_dir = Path('./features/').resolve()\n",
    "out_dir.mkdir(parents=True, exist_ok=True)\n",
    "# Local import\n",
    "from utils import *\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Helper function\n",
    "def auroc_score(score_in, score_out):\n",
    "    if type(score_in) == torch.Tensor:\n",
    "        score_in = score_in.cpu().numpy()\n",
    "        score_out = score_out.cpu().numpy()\n",
    "    labels = np.concatenate((np.ones_like(score_in), np.zeros_like(score_out)))\n",
    "    return roc_auc_score(labels, np.concatenate((score_in, score_out))) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II. Get the visual features of the CLIP model\n",
    "\n",
    "- We will use `CIFAR100` as the in-distribution, and `CIFAR10` as the out-distribution.\n",
    "- When you are only loading the visual CLIP backbone, you must remove the final linear layer that projects the features to the shared feature space of the image-text encoder.\n",
    "- Load the data, compute the visual features and save them in the `features` folder.\n",
    "- For the in-distribution you need both the train and test split, while for the out-distribution, we will only use the validation split.\n",
    "\n",
    "\n",
    "### Optional structure\n",
    "\n",
    "```python\n",
    "def load_datasets(indist=\"CIFAR100\", ood=\"CIFAR10\", batch_size=256, tranform=None):\n",
    "    # ....\n",
    "    return indist_train_loader, indist_test_loader, ood_loader\n",
    "\n",
    "# visual is a boolean that controls whether the visual backbone is only returned or the whole CLIP model\n",
    "def get_model(visual, name, pretrained)\n",
    "    # .....\n",
    "    if visual:\n",
    "        return backbone, preprocess\n",
    "    return model, preprocess, tokenizer\n",
    "    \n",
    "# Load everything .......\n",
    "\n",
    "feats, labels = get_features(backbone, dl, device)\n",
    "# Save features \n",
    "# ....\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 31 lines of code)\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# feature test\n",
    "for name, N in [('cifar100_train', 50000), ('cifar100_test', 10000), ('cifar10_test', 10000)]:\n",
    "    feats = torch.load(f'features/{name}_feats.pt')\n",
    "    labels = torch.load(f'features/{name}_labels.pt')\n",
    "    assert feats.shape == (N, 1024)\n",
    "    assert labels.shape == (N,)\n",
    "print('Success!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III. Compute the k-NN similarity as the OOD score\n",
    "\n",
    "- For each test image of in and out distribution compute the top-1 cosine similarity and use it as OOD score.\n",
    "- Report the resulting AUROC score.\n",
    "- Note: Use the image features and not the images!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def OOD_classifier_knn(train_features, test_features, k=1):\n",
    "    ### START CODE HERE ### (≈ 13 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return cos_sim \n",
    "\n",
    "# load the computed features and compute scores\n",
    "### START CODE HERE ### (≈ 5 lines of code)\n",
    "indist_train = ...\n",
    "indist_test = ...\n",
    "ood_test = ...\n",
    "score_in = ...\n",
    "score_out =  ...\n",
    "### END CODE HERE ###\n",
    "print(f'CIFAR100-->CIFAR10 AUROC: {auroc_score(score_in, score_out):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "CIFAR100-->CIFAR10 AUROC: 83.55\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Compute MSP using the text encoder and the label names\n",
    "\n",
    "We will now consider the case where the in-distribution label names are available.\n",
    "\n",
    "Your task is to apply zero-shot classification and get the maximum softmax probability (MSP) as the OOD score.\n",
    "\n",
    "In short:\n",
    "- compute image and text embeddings\n",
    "- compute the image-test similarity matrix (logits)\n",
    "- apply softmax to the logits for each image to get a probability distribution of the classes.\n",
    "- compute maximum softmax probability (MSP)\n",
    "\n",
    "- `Note`: After loading the saved image features you need to apply the linear projection layer from the visual backbone of CLIP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional to use\n",
    "def compute_logits(model, text_embs, img_embs, device):\n",
    "    ### START CODE HERE ### (≈ 5 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return logits\n",
    "\n",
    "### optional to use\n",
    "def compute_text_embeds(model, class_tokens):\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return text_embs\n",
    "\n",
    "def compute_msp(label_names, model, class_tokens, indist_test, ood_test, device):\n",
    "    ### START CODE HERE ### (≈ 7 lines of code)\n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "    return score_in, score_out\n",
    "        \n",
    "\n",
    "# Load model and features\n",
    "### START CODE HERE ### (≈ 4 lines of code)\n",
    "indist_test = ...\n",
    "ood_test = ...\n",
    "model, preprocess, tokenizer = ...\n",
    "model = ...\n",
    "### END CODE HERE ###\n",
    "\n",
    "### Provided \n",
    "label_names = torchvision.datasets.CIFAR100(root='../data', train=True, download=True).classes\n",
    "prompts = ['an image of a ' + lab.replace('_', ' ') for lab in label_names]\n",
    "class_tokens = tokenizer(label_names).to(device)\n",
    "score_in, score_out = compute_msp(label_names, model, class_tokens, indist_test, ood_test, device)\n",
    "print(f'CIFAR100-->CIFAR10 AUROC: {auroc_score(score_in, score_out):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "CIFAR100-->CIFAR10 AUROC: 76.38\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Linear probing on the pseudolabels\n",
    "\n",
    "- Your task is to train a linear layer using the CLIP pseudolabels as targets.\n",
    "- The pseudolabels are the argmax of the logits computed above, i.e., take the class with the maximum probability as the class label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_score_probe_msp(lin_layer, indist_loader, ood_loader, device):\n",
    "    \"\"\"\n",
    "    Computes the MSP scores for a linear layer for both in- and out- distribution.\n",
    "    \"\"\"\n",
    "    ### START CODE HERE ### (≈ 4 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return score_in, score_out\n",
    "\n",
    "### START CODE HERE ### (≈ 17 lines of code) \n",
    "# get CLIP model\n",
    "# get text embeds from label names\n",
    "\n",
    "# load features\n",
    "\n",
    "# compute CLIP logits of image features based on text encoder\n",
    "\n",
    "# get target pseudo labels from CLIP logits\n",
    "\n",
    "# create dataset and dataloaders for linear probing\n",
    "\n",
    "### END CODE HERE ###\n",
    "\n",
    "# The code below is provided based on our implementation. Optional to use!\n",
    "# Run linear probing\n",
    "embed_dim = train_dataset[0][0].shape[0]\n",
    "lin_layer = nn.Linear(embed_dim, 100).to(device)\n",
    "optimizer = torch.optim.Adam(lin_layer.parameters(), lr=1e-3)\n",
    "num_epochs = 20\n",
    "dict_log = linear_eval(lin_layer, optimizer, num_epochs, train_loader, val_loader, device)\n",
    "# compute MSP scores\n",
    "lin_layer = load_model(lin_layer, \"CLIP_best_max_train_acc.pth\")\n",
    "ood_dataset = torch.utils.data.TensorDataset(ood_test, torch.zeros(ood_test.shape[0], dtype=torch.long))\n",
    "ood_loader = torch.utils.data.DataLoader(ood_dataset, batch_size=128, shuffle=False, drop_last=False)\n",
    "score_in, score_out = compute_score_probe_msp(lin_layer, val_loader, ood_loader, device)\n",
    "print(f'CIFAR100-->CIFAR10 AUROC: {auroc_score(score_in, score_out):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "AUROC may slightly vary due to random initialization of linear probing.\n",
    "```\n",
    "CIFAR100-->CIFAR10 AUROC: 74.81\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI. Mahalanobis distance as OOD score\n",
    "- Use the output of the linear layer from task 4 as features to compute the Mahalanobis distance and the relative Mahalanobis distance.\n",
    "- To compute the Mahalanobis distance group the features by their pseudolabels and compute the mean and covariance matrix for each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### optional to use\n",
    "@torch.no_grad()\n",
    "def calc_maha_distance(embeds, means_c, inv_cov_c):\n",
    "    ### START CODE HERE ### (≈ 3 lines of code)\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return dist\n",
    "\n",
    "def OOD_classifier_maha(train_embeds_in, train_labels_in, test_embeds_in, test_embeds_outs, num_classes,\n",
    "                        relative=False):\n",
    "    # optional to use our code!\n",
    "    class_covs = []\n",
    "    class_means = []\n",
    "    used_classes = 0\n",
    "    if type(train_labels_in) == torch.Tensor:\n",
    "        train_labels_in = train_labels_in.cpu().numpy()\n",
    "    if type(train_embeds_in) == torch.Tensor:\n",
    "        train_embeds_in = train_embeds_in.cpu().numpy()\n",
    "        test_embeds_in = test_embeds_in.cpu().numpy()\n",
    "        test_embeds_outs = test_embeds_outs.cpu().numpy()\n",
    "    ### START CODE HERE ### (≈ 23 lines of code)\n",
    "    # calculate class-wise means and covariances\n",
    "    \n",
    "    # estimating the global std from train data\n",
    "   \n",
    "    # RMD: subtracting the average train score if relative is True\n",
    "   \n",
    "    # Get OOD score for each datapoint\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "    return scores_in, scores_out\n",
    "\n",
    "# The code below is provided based on our implementation. Optional to use!\n",
    "num_classes = 100\n",
    "lin_layer = load_model(lin_layer, \"CLIP_best_max_train_acc.pth\")\n",
    "logits_indist_train, indist_pseudolabels_train = get_features(lin_layer, train_loader, device)\n",
    "logits_indist_test, indist_pseudolabels_test = get_features(lin_layer, val_loader, device)\n",
    "logits_ood, _ = get_features(lin_layer, ood_loader, device)\n",
    "# convert to numpy\n",
    "indist_pseudolabels_train = indist_pseudolabels_train.cpu().numpy()\n",
    "indist_pseudolabels_test = indist_pseudolabels_test.cpu().numpy()\n",
    "logits_indist_train = logits_indist_train.cpu().numpy()\n",
    "logits_indist_test = logits_indist_test.cpu().numpy()\n",
    "logits_ood = logits_ood.cpu().numpy()\n",
    "\n",
    "# run OOD classifier based on mahalanobis distance\n",
    "scores_in, scores_out = OOD_classifier_maha(logits_indist_train, indist_pseudolabels_train, \n",
    "                                            logits_indist_test, logits_ood, num_classes, relative=False)\n",
    "print(f'Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_in, scores_out):.2f}')\n",
    "scores_in, scores_out = OOD_classifier_maha(logits_indist_train, indist_pseudolabels_train, \n",
    "                                            logits_indist_test, logits_ood, num_classes, relative=True)\n",
    "print(f'Relative Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_in, scores_out):.2f}')                                    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "(can differ based on linear probing performance)\n",
    "\n",
    "```\n",
    "Maha: CIFAR100-->CIFAR10 AUROC: 83.31\n",
    "Relative Maha: CIFAR100-->CIFAR10 AUROC: 80.88\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Mahalanobis distance using the real labels without linear probing\n",
    "- Again, compute the (relative) Mahalanobis distance as OOD score\n",
    "- This time, instead of using the pseudolabels and output of the linear probing layer, use the real labels of the training data and the features computed in task 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 7 lines of code)\n",
    "# load features\n",
    "\n",
    "# load labels\n",
    "\n",
    "# run OOD classifier based on mahalanobis distance\n",
    "\n",
    "### END CODE HERE ###\n",
    "print(f'Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_md_in, scores_md_out):.2f}')\n",
    "print(f'Relative Maha: CIFAR100-->CIFAR10 AUROC: {auroc_score(scores_rmd_in, scores_rmd_out):.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "```\n",
    "Maha: CIFAR100-->CIFAR10 AUROC: 71.71\n",
    "Relative Maha: CIFAR100-->CIFAR10 AUROC: 84.93\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII. K-means clusters combined with Mahalanobis distance\n",
    "\n",
    "The paper [SSD: A Unified Framework for Self-Supervised Outlier Detection](https://arxiv.org/abs/2103.12051) has proposed another unsupervised method for OOD detection. Instead of using the (real or pseudo) labels as class-wise means, we will now use the obtained clusters as found be kmeans. In more detail:\n",
    "\n",
    "- Find k=10,50,100 clusters using Kmeans on the in-distribution training data (you can use the sklearn KMeans implementation).\n",
    "- Get the cluster centers.\n",
    "- Use them as class-wise means for the mahalanobis distance classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code below is provided based on our implementation. Optional to use!\n",
    "# load features - modify names if you use different names\n",
    "indist_train = torch.load('features/cifar100_train_feats.pt').cpu().numpy()\n",
    "indist_test = torch.load('features/cifar100_test_feats.pt').cpu().numpy()\n",
    "ood_test = torch.load('features/cifar10_test_feats.pt').cpu().numpy()\n",
    "results_md = []\n",
    "results_rmd = []\n",
    "for N in [10,50,100]:\n",
    "    ### START CODE HERE ### (≈ 7 lines of code)\n",
    "   \n",
    "    ### END CODE HERE ###\n",
    "    print(f'Kmeans (k={N}) + RMD: CIFAR100-->CIFAR10 AUROC: {auroc_rmd:.2f}')\n",
    "    print(\"-\"*100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "Can differ based on KMenas performance.\n",
    "```\n",
    "Kmeans (k=10) + MD: CIFAR100-->CIFAR10 AUROC: 67.87\n",
    "Kmeans (k=10) + RMD: CIFAR100-->CIFAR10 AUROC: 42.38\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Kmeans (k=50) + MD: CIFAR100-->CIFAR10 AUROC: 72.18\n",
    "Kmeans (k=50) + RMD: CIFAR100-->CIFAR10 AUROC: 58.73\n",
    "----------------------------------------------------------------------------------------------------\n",
    "Kmeans (k=100) + MD: CIFAR100-->CIFAR10 AUROC: 72.84\n",
    "Kmeans (k=100) + RMD: CIFAR100-->CIFAR10 AUROC: 68.59\n",
    "----------------------------------------------------------------------------------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RK4j971u_4yu"
   },
   "source": [
    "That's the end of this exercise. If you reached this point, **congratulations**!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "[Exercise 3 solution] - Self-distillation on CIFAR100 .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15451b89dea54127867d368514cfea78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d335008de124ef1890de5087a8254f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15451b89dea54127867d368514cfea78",
      "placeholder": "​",
      "style": "IPY_MODEL_a1a228b8e820488aa9d7a39326832b43",
      "value": ""
     }
    },
    "5c84f8f441eb4b65a2947a8b0076b78c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "947edc38a98549e79c0906847f20560b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bb9f2adc1f4480ba6ae6b390eda4521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0db04ae470a41c098cb9b59a67e899b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a228b8e820488aa9d7a39326832b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a83800177edb46bdb5789bd0507c55b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bb9f2adc1f4480ba6ae6b390eda4521",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6ea4f8b6c304a279b56769a81897f5f",
      "value": 169001437
     }
    },
    "aca8ccf946814899be23fad49d4c4ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0db04ae470a41c098cb9b59a67e899b",
      "placeholder": "​",
      "style": "IPY_MODEL_947edc38a98549e79c0906847f20560b",
      "value": " 169001984/? [00:10&lt;00:00, 16869945.76it/s]"
     }
    },
    "ae807dfd8be647d29b923f286caa47c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d335008de124ef1890de5087a8254f8",
       "IPY_MODEL_a83800177edb46bdb5789bd0507c55b1",
       "IPY_MODEL_aca8ccf946814899be23fad49d4c4ecb"
      ],
      "layout": "IPY_MODEL_5c84f8f441eb4b65a2947a8b0076b78c"
     }
    },
    "d6ea4f8b6c304a279b56769a81897f5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
