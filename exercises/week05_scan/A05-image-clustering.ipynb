{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-uirVvNW-yY"
   },
   "source": [
    "HHU Deep Learning, SS2022/23, 12.05.2023, Prof. Dr. Markus Kollmann\n",
    "\n",
    "Lecturers and Tutoring is done by Tim Kaiser, Nikolas Adaloglou and Felix Michels.\n",
    "\n",
    "# Assignment 05 - Image Clustering\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Imports, basic utils, augmentations\n",
    "2. Load the pretrained MoCO model ResNet50 pretrained on ImageNet\n",
    "3. Compute the k-means clustering accuracy using the learned representations\n",
    "4. T-SNE visualization of features\n",
    "5. Compute the 50-NN\n",
    "6. Write a new dataset class to load image pairs\n",
    "7. Implement the SCAN loss\n",
    "8. Implement the PMI loss. Train the clustering head and compute the validation accuracy\n",
    "9. Pretraining code. (Provided, no need to change something here!)\n",
    "10. Train with SCAN and PMI using the KNN pairs\n",
    "11. Get cluster assignments and evaluate cluster accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction \n",
    "\n",
    "Image clustering in deep learning can be mathematically described as a process of partitioning a set of images, X, into K clusters, where K is a user-defined parameter representing the number of desired clusters.\n",
    "\n",
    "Let V(X) be the visual feature representation of the images in X, obtained using a deep learning algorithm such as a convolutional neural network (CNN). Each image in X is transformed into a feature vector in V(X), where the dimensions correspond to the learned features of the CNN.\n",
    "\n",
    "Image clustering is a task in deep learning where an algorithm is used to group similar images together based on their visual characteristics. Ideally, images with similar ground truth labels will belong in the same cluster.\n",
    "\n",
    "The goal of image clustering is to automatically categorize large sets of images into smaller subsets based on their similarities, which can help in organizing and managing large image datasets.\n",
    "\n",
    "To accomplish this task, deep learning algorithms use complex mathematical models to analyze and identify patterns within the images, and then group the images that share these patterns into clusters. This process can be useful in a variety of applications, such as image recognition, image search, and content-based image retrieval.\n",
    "\n",
    "\n",
    "[SimCLR Paper](https://arxiv.org/abs/2002.05709)\n",
    "\n",
    "[MoCo Paper](https://arxiv.org/abs/1911.05722)\n",
    "\n",
    "[SCAN Paper](https://arxiv.org/abs/2005.12320v2)\n",
    "\n",
    "[TEMI](https://arxiv.org/abs/2303.17896)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw5K7r5SQDca"
   },
   "source": [
    "# Part I. Imports, basic utils, augmentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1SIad0uuHBlv"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision.models as models\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets import STL10\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import tqdm\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.cluster import KMeans\n",
    "# Local imports\n",
    "from utils import *\n",
    "\n",
    "os.makedirs(\"./figs\", exist_ok=True)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: Load the pretrained MoCO model ResNet50 pretrained on ImageNet\n",
    "\n",
    "[Weights are available in this link](https://dl.fbaipublicfiles.com/moco/moco_checkpoints/moco_v2_800ep/moco_v2_800ep_pretrain.pth.tar)\n",
    "\n",
    "You can download the weight by running the terminal command:\n",
    "\n",
    "`$ wget link_to_model_weights`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_moco_model(pretrained_path = \"./moco_v2_800ep_pretrain.pth.tar\"):\n",
    "    ### START CODE HERE ### (≈ 11 lines of code)\n",
    "    ckpt = torch.load(pretrained_path, map_location='cpu')\n",
    "    print(ckpt.keys(), ckpt[\"arch\"], ckpt[\"epoch\"])\n",
    "    state_dict = ckpt[\"state_dict\"]\n",
    "    state_dict_new = dict()\n",
    "    for key in state_dict.keys():\n",
    "        new_key = key.replace(\"module.encoder_q.\",\"\")\n",
    "        state_dict_new[new_key] = state_dict[key]\n",
    "    model = getattr(models, ckpt[\"arch\"])(pretrained=False)\n",
    "    model.fc = nn.Identity()\n",
    "    msg = model.load_state_dict(state_dict_new, strict=False)\n",
    "    print(\"Loaded model with message:\", msg)\n",
    "    ### END CODE HERE ###\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "encoder = load_moco_model() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "There should be no missing keys, while loading the model. There may be some unexpected keys based on your implementation.\n",
    "\n",
    "```python\n",
    "Loaded model with message: _IncompatibleKeys(missing_keys=[], unexpected_keys=['fc.0.weight', 'fc.0.bias', 'fc.2.weight', 'fc.2.bias'])\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4v-Qg5Xpk2Bv"
   },
   "source": [
    "# Part III: Compute the k-means clustering accuracy using the learned representations\n",
    "\n",
    "\n",
    "- Compute the frozen features representations of the backbone model.\n",
    "- Compute the accuracy both for the `train` and `test` split using Kmeans.\n",
    "\n",
    "Hint: you may use the function 'compute_clustering_metrics' defined in utils.py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FeP4ZuZpsyOp"
   },
   "outputs": [],
   "source": [
    "transf = T.Compose([\n",
    "            T.ToTensor(),\n",
    "            T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])\n",
    "### START CODE HERE ### (≈>10 lines of code)\n",
    "\n",
    "# compute features for train and val\n",
    "\n",
    "#Fitt k-means.....\n",
    "\n",
    "\n",
    "# compute clustering metrics for train and val\n",
    "train_acc = compute_clustering_metrics(train_labels.cpu().numpy(), train_preds,min_samples_per_class=10)[0]\n",
    "val_acc = compute_clustering_metrics(train_labels.cpu().numpy(), train_preds, min_samples_per_class=10)[0]\n",
    "\n",
    "### END CODE HERE ###\n",
    "print(f\"Train acc: {train_acc:.2f}, Val acc: {val_acc:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "`Train acc: 53.64, Val acc: 53.64`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. T-SNE visualization of features\n",
    "\n",
    "As in the previous exercise, check the results of linear probing on the supervised training split and the T-SNE visualization.\n",
    "\n",
    "Code for the T-SNE visualization exists in `utils.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 line of code)\n",
    "# TSNE plot\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Compute the 50-NN\n",
    "\n",
    "- Load the train features\n",
    "- Use the cosine similarity\n",
    "- Compute the k=50 nearset neiboughrs(NN) on the feature space of the pretrained ResNet50\n",
    "- save the indices of the k-NN.\n",
    "- Visualize the top 5 NN for a couple of images (~10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Provided but optional to use!\n",
    "class_names = torchvision.datasets.STL10(root='../data').classes\n",
    "def vizualize_pairs(indices, true_labels, train_ds):\n",
    "    # Visualize the reference image and its 7 nearest neighbors\n",
    "    ref_ids = [0, 100, 200, 300, 400, 500, 600, 700, 800, 900]\n",
    "    nn_viz = 6 \n",
    "    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n",
    "    plt.figure(figsize = (22,22))\n",
    "    ax = plt.gca()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "    for c, ref in enumerate(ref_ids):\n",
    "        knns = indices[ref, :nn_viz]\n",
    "        imgs_to_viz = [train_ds[ref][0]]\n",
    "        true_labels = [train_ds[ref][1]]\n",
    "        for i in knns:\n",
    "            imgs_to_viz.append(train_ds[i][0])\n",
    "            true_labels.append(train_ds[i][1])\n",
    "        # show the images\n",
    "        for j in range(nn_viz):\n",
    "            label = int(true_labels[j])\n",
    "            plt.subplot(len(ref_ids), nn_viz, (c*nn_viz)+(j+1))\n",
    "            imshow(imgs_to_viz[j])\n",
    "            plt.title(f\"{class_names[label]}, Label {label}\", fontsize = 10)\n",
    "            ax = plt.gca()\n",
    "            ax.get_xaxis().set_visible(False)\n",
    "            ax.get_yaxis().set_visible(False)\n",
    "    plt.savefig(f'./figs/knn_viz', bbox_inches = \"tight\", dpi = 500) \n",
    "\n",
    "### START CODE HERE ### (≈ 10 line of code)\n",
    "\n",
    "# compute the similarity matrix\n",
    "\n",
    "# take top k similar images\n",
    "\n",
    "\n",
    "# save the indices\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI. Write a new dataset class to load image pairs\n",
    "\n",
    "- The new dataset class will inherit from `torch.utils.data.Dataset`\n",
    "- It will return the representations of 2 images that are in the 50-NN (randomly sampled)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE (≈ 12 lines of code)\n",
    "class PairSTL10(torch.utils.data.Dataset):\n",
    "    def __init__(self, indices_path=\"./knn_indices.pth\", embeds_path=\"./train_feats.pth\", l2_normalize=True):\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "\n",
    "### END CODE HERE\n",
    "    \n",
    "def test_get_pair():\n",
    "    dataset = PairSTL10()\n",
    "    emb1, emb2 = dataset[16]\n",
    "    print(emb1.shape, emb2.shape)\n",
    "    assert emb1.shape==emb2.shape \n",
    "\n",
    "test_get_pair()\n",
    "train_loader = torch.utils.data.DataLoader(PairSTL10(), batch_size=128, shuffle=True, num_workers=4)\n",
    "data_batch = next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Implement the SCAN loss\n",
    "\n",
    "Check the SCAN paper, specifically Eq.2 for details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SCAN(torch.nn.Module):\n",
    "    def __init__(self, alpha=1):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "\n",
    "    def forward(self, proj_1, proj_2):\n",
    "        # START CODE HERE (≈ 6 line of code)\n",
    "        \n",
    "        # dot product\n",
    "        \n",
    "\n",
    "        # self-entropy regularization\n",
    "\n",
    "        ### END CODE HERE\n",
    "\n",
    "def test_scan():\n",
    "    torch.manual_seed(99)\n",
    "    scan = SCAN(alpha=1)\n",
    "    proj_1 = torch.randn(100, 128)\n",
    "    proj_2 = torch.randn(100, 128)\n",
    "    loss = scan(proj_1, proj_2)\n",
    "    print(loss)\n",
    "    assert loss.shape==torch.Size([])\n",
    "test_scan()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "For alpha=1, output = `tensor(0.0275)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII. Implement the PMI loss. Train the clustering head and compute the validation accuracy\n",
    "\n",
    "Implement the PMI loss based on eq 6,7,8 from the paper https://arxiv.org/pdf/2303.17896.pdf\n",
    "\n",
    "As a side note we didnt use the symmetrized version of the loss in the exercise: Loss = -PMI, don't forget the sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PMI(torch.nn.Module):\n",
    "    def __init__(self, gamma=1, momentum=0.99, temp=0.1):\n",
    "        super().__init__()\n",
    "        self.gamma = gamma\n",
    "        self.temp = temp\n",
    "        self.center  = None\n",
    "        self.momentum = momentum\n",
    "    \n",
    "    # START CODE HERE (≈ 6 line of code)\n",
    "    @torch.no_grad()\n",
    "    def update_ema(self, output):\n",
    "        \"\"\"\n",
    "        Update exponential moving average of the center (denominator)\n",
    "        \"\"\"\n",
    "        \n",
    "    def forward(self, proj_1, proj_2):\n",
    "        \n",
    "    ### END CODE HERE\n",
    "\n",
    "def test_pmi():\n",
    "    torch.manual_seed(99)\n",
    "    criterion = PMI(gamma=1)\n",
    "    proj_1 = torch.rand(100, 128)\n",
    "    proj_2 = torch.rand(100, 128)\n",
    "    loss = criterion(proj_1, proj_2)\n",
    "    print(loss)\n",
    "    assert loss.shape==torch.Size([])\n",
    " \n",
    "test_pmi()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results \n",
    "\n",
    "`tensor(0.0738)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IX. PROVIDED: Pretraining code\n",
    "\n",
    "This part is provided, but please take a look and identify what is changing compared to the standard train loop.\n",
    "\n",
    "You don't need to code something here, unless there is some inconsitency with the previous parts of the code.\n",
    "\n",
    "Still, this code works in our proposed solution and it's your job to modify it if it doesnt work well with the previous code based on your implementations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy \n",
    "\n",
    "\n",
    "def pretrain(model, optimizer, num_epochs, train_loader, criterion, device, prefix=\"scan\", model_ema=False):\n",
    "    dict_log = {\"train_loss\":[]}\n",
    "    best_loss = 1e8\n",
    "    model = model.to(device)\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        loss_curr_epoch = pretrain_one_epoch(model, optimizer, train_loader, criterion, device, model_ema=model_ema)\n",
    "        msg = (f'Ep {epoch}/{num_epochs}: || Loss: Train {loss_curr_epoch:.3f}')\n",
    "        pbar.set_description(msg)\n",
    "        dict_log[\"train_loss\"].append(loss_curr_epoch)\n",
    "        if loss_curr_epoch < best_loss:\n",
    "            best_loss = loss_curr_epoch\n",
    "            save_model(model, f'{prefix}_best_model_min_val_loss.pth', epoch, optimizer, best_loss)   \n",
    "    return dict_log\n",
    "\n",
    "class EMA():\n",
    "    def __init__(self, alpha, student):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.teacher = copy.deepcopy(student)\n",
    "        for p in self.teacher.parameters():\n",
    "            p.requires_grad = False\n",
    "    \n",
    "    def update_average(self, old, new):\n",
    "        if old is None:\n",
    "            return new\n",
    "        return old * self.alpha + (1 - self.alpha) * new\n",
    "    \n",
    "    def update_teacher(self, student):\n",
    "        for ema_params, student_params in zip(self.teacher.parameters(), student.parameters()):\n",
    "            old_weight, student_weight = ema_params.data, student_params.data\n",
    "            ema_params.data = self.update_average(old_weight, student_weight)\n",
    "\n",
    "\n",
    "def pretrain_one_epoch(model, optimizer, train_loader, criterion, device, model_ema=False):\n",
    "    \"\"\"\n",
    "    model: the model to train\n",
    "    optimizer: the optimizer to use\n",
    "    train_loader: the train loader\n",
    "    criterion: the loss function, PMI or SCAN\n",
    "    device: the device to use\n",
    "    model_ema: whether to use EMA or not\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "    loss_step = []\n",
    "    if model_ema:\n",
    "        ema = EMA(0.99, model)\n",
    "    for data in train_loader:\n",
    "        # Move the data to the GPU\n",
    "        img1, img2 = data\n",
    "        img1, img2 = img1.to(device), img2.to(device)\n",
    "        p1 = model(img1)\n",
    "        p2 = ema.teacher(img2) if model_ema else model(img2)\n",
    "        loss = criterion(p1, p2)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        loss_step.append(loss.item())\n",
    "        if model_ema:\n",
    "            ema.update_teacher(model)\n",
    "    loss_curr_epoch = np.mean(loss_step)\n",
    "    return loss_curr_epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part X. Train with SCAN and PMI using the KNN pairs\n",
    "\n",
    "- Load the data using the implemented dataloader\n",
    "- Create a clustering head\n",
    "- Train head using Adam: optimizer, lr=1e-4, weight_decay=1e-6 for 150 epochs.\n",
    "- Train with SCAN and PMI and compare them with k-means.\n",
    "\n",
    "You can use the pretrain function:\n",
    "```python\n",
    "dict = pretrain(head, optimizer, num_epochs, train_loader, criterion, ......)\n",
    "```\n",
    "\n",
    "Training should **not** take more than 5 minutes for both models.\n",
    "\n",
    "We used: `PMI(gamma=0.65, momentum=0.9, temp=0.1)` for PMI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (>15 line of code)\n",
    "# SCAN\n",
    "criterion = SCAN(alpha=......)\n",
    "\n",
    "optimizer = torch.optim.Adam(scan_head.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "dict_log_scan = pretrain(scan_head, optimizer, num_epochs, train_loader, criterion, device, prefix=\"scan\")\n",
    "\n",
    "# PMI\n",
    "criterion = PMI(.....)\n",
    "\n",
    "optimizer = torch.optim.Adam(pmi_head.parameters(), lr=1e-4, weight_decay=1e-6)\n",
    "dict_log_pmi = pretrain(pmi_head, optimizer, num_epochs, train_loader, criterion, device, prefix=\"pmi\", model_ema=True)\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part XI. Get cluster assignments and evaluate cluster accuracy\n",
    "\n",
    "- Load the model trained with both objectives.\n",
    "- Predict cluster assignments.\n",
    "- Compute the clustering accuracy using `compute_clustering_metrics`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate_clustering(model):\n",
    "    model.eval()\n",
    "    val_feats, val_labels = torch.load(\"val_feats.pth\"), torch.load(\"val_labels.pth\")\n",
    "    train_feats, train_labels = torch.load(\"train_feats.pth\"), torch.load(\"train_labels.pth\")\n",
    "    ### START CODE HERE ### (≈ 10 lines of code)\n",
    "    # normalize feats\n",
    "    \n",
    "    # load features and compute logits\n",
    "\n",
    "\n",
    "    # compute metrics\n",
    "    print(\"Unique preds\", np.unique(train_preds), np.unique(val_preds))\n",
    "    metrics_train = compute_clustering_metrics(train_labels.cpu().numpy(), train_preds, min_samples_per_class=10)\n",
    "    metrics_val = compute_clustering_metrics(val_labels.cpu().numpy(), val_preds,min_samples_per_class=10)\n",
    "    return metrics_train[0], metrics_val[0]\n",
    "    ### END CODE HERE ###\n",
    "    \n",
    "\n",
    "# Given but you may need to MODIFY the paths!!!!\n",
    "n_clusters = 10\n",
    "### START CODE HERE ### (4 lines of code)\n",
    "model = ....\n",
    "model_scan = load_model(model, \"./scan_best_model_min_val_loss.pth\")\n",
    "model = ....\n",
    "model_pmi = load_model(model, \"./pmi_best_model_min_val_loss.pth\")\n",
    "### END CODE HERE ###\n",
    "train_acc, val_acc = evaluate_clustering(model_scan)\n",
    "print(f\"SCAN: Train acc: {train_acc:.3f}, Val acc: {val_acc:.3f}\")\n",
    "train_acc, val_acc = evaluate_clustering(model_pmi)\n",
    "print(f\"PMI: Train acc: {train_acc:.3f}, Val acc: {val_acc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results:\n",
    "Current best scores! Results may slightly vary between runs.\n",
    "```\n",
    "Model ./scan_best_model_min_val_loss.pth is loaded from epoch 148 , loss -22.383880043029784\n",
    "Model ./pmi_best_model_min_val_loss.pth is loaded from epoch 129 , loss -2.0719790697097777\n",
    "Unique preds [0 1 2 3 4 5 6 7 8 9] [0 1 2 3 4 5 6 7 8 9]\n",
    "SCAN: Train acc: 74.380, Val acc: 74.450\n",
    "Unique preds [0 1 2 3 4 5 6 7 8 9] [0 1 2 3 4 5 6 7 8 9]\n",
    "PMI: Train acc: 77.280, Val acc: 78.238\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion and Bonus reads\n",
    "\n",
    "That's the end of this exercise. If you reached this point, congratulations!\n",
    "\n",
    "Additional things to to (Optional):\n",
    "\n",
    "- Plot the histogram of class assignments for SCAN and PMI\n",
    "- Compute the mean and median max softmax probability for SCAN and PMI\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "machine_shape": "hm",
   "name": "[Exercise 4] - SimCLR Resnet18 Solution.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
