{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CCfZVwhwMIYN"
   },
   "source": [
    "HHU Deep Learning, SS2022/23, 19.05.2023, Prof. Dr. Markus Kollmann\n",
    "\n",
    "Lecturers and Tutoring is done by Tim Kaiser, Nikolas Adaloglou and Felix Michels.\n",
    "\n",
    "# Assignment 07 - Knowledge distillation on CIFAR100 with Vision Transformers\n",
    "\n",
    "\n",
    "Copyright © 2023 Nikolas Adaloglou, Tim Kaiser and Felix Michels\n",
    "\n",
    "Link to Imagenet-pretrained + fine-tuned teacher model on CIFAR100: https://uni-duesseldorf.sciebo.de/s/Y6yGbC9AHpxMKBJ\n",
    "\n",
    "\n",
    "## Contents\n",
    "\n",
    "1. Imports and preparation\n",
    "2. Load the timm models\n",
    "3. Applying image augmentations with torchvision\n",
    "4. Fine-tune the teacher model on CIFAR100\n",
    "5. Train ViT-tiny (student) from scratch (random init) with cross-entropy (without knowledge distilation)\n",
    "6. Implement knowledge distillation loss\n",
    "7. Implement MultiScaleData loading\n",
    "8. Adjust training code to support knowledge distillation from the teacher model\n",
    "9. Train student with knowledge distillation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following method is partially based on the paper: [\"Be Your Own Teacher: Improve the Performance of Convolutional Neural\n",
    "Networks via Self Distillation\"](https://arxiv.org/pdf/1905.08094.pdf)\n",
    "\n",
    "Briefly, knowledge distillation simply trains a new randomly initialized model to match the prediction of another trained model.\n",
    "\n",
    "The output of the so-called teacher model is some mixed version of a set of real labels, i.e. 88% cat, 7% tiger, 5% dog.\n",
    "\n",
    "\n",
    "#### Overview\n",
    "The teacher model is usually a larger model pretrained on the same dataset. However, for the purpose of this exercise we will use a larger imagenet-pretrained model (ViT-base), fine-tune it on cifar100. This will be our teacher. \n",
    "\n",
    "We will then try to improve a much smaller model trained from scratch with the losses proposed in https://arxiv.org/pdf/1905.08094.pdf , equation 2 and equation 3.\n",
    "\n",
    "\n",
    "`Note`: I used the pretrained models from timm, you can install it with `!pip install timm`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I. Imports and preparation\n",
    "\n",
    "\n",
    "Below we provide the imports and some necessary data functionalities. We will experiment with CIFAR100 this time!\n",
    "\n",
    "You may need to change the path (root='../data') where the data will be downloaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 177,
     "referenced_widgets": [
      "ae807dfd8be647d29b923f286caa47c4",
      "5c84f8f441eb4b65a2947a8b0076b78c",
      "4d335008de124ef1890de5087a8254f8",
      "a83800177edb46bdb5789bd0507c55b1",
      "aca8ccf946814899be23fad49d4c4ecb",
      "a1a228b8e820488aa9d7a39326832b43",
      "15451b89dea54127867d368514cfea78",
      "d6ea4f8b6c304a279b56769a81897f5f",
      "9bb9f2adc1f4480ba6ae6b390eda4521",
      "947edc38a98549e79c0906847f20560b",
      "a0db04ae470a41c098cb9b59a67e899b"
     ]
    },
    "id": "juADOUzcL52G",
    "outputId": "3423a982-c88b-49ce-9af1-76a04f2d07a3"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import torch\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import torchvision\n",
    "import timm\n",
    "import torch.optim as optim\n",
    "\n",
    "# local imports\n",
    "from utils import *\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "reproducibility(99)\n",
    "\n",
    "def get_transform_plain():\n",
    "    mean = torch.tensor([0.485, 0.456, 0.406], dtype=torch.float32)\n",
    "    std = torch.tensor([0.229, 0.224, 0.225], dtype=torch.float32)\n",
    "    normalize = transforms.Normalize(mean.tolist(), std.tolist())\n",
    "    return transforms.Compose(\n",
    "                    [transforms.ToTensor(),\n",
    "                    normalize])\n",
    "\n",
    "def load_cifar100_data(transform=None, train=True, batch_size=64, num_workers=2, shuffle=False):\n",
    "    pin = True if train else False\n",
    "    if transform is None:\n",
    "        transform = get_transform_plain()\n",
    "        \n",
    "    dataset = torchvision.datasets.CIFAR100(root='../data',  transform=transform, train=train, download=True)\n",
    "    loader = torch.utils.data.DataLoader(dataset, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=shuffle,\n",
    "                                         num_workers=num_workers,\n",
    "                                         pin_memory=pin)\n",
    "    return loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tuVDgGlmZz6O"
   },
   "source": [
    "# Part II.  Load the timm models\n",
    "\n",
    "- Teacher ViT-B: use the supervised imagenet-1k pretrained weights of ViT-Base from `timm` we will use that model for supervised fine-tuning.\n",
    "- Student: create a vision transformer for CIFAR100 images, patch_size of 4, embeding dimension of 192, 12 layers with 3 heads per layer, and prenorm: layer normalization should be applied before the attention and the mlp. This is the model that will be trained from scratch. Use random weights!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 3 lines of code) \n",
    "teacher_vitb = timm.create_model(...)\n",
    "model_args = ...\n",
    "student = ...\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LQldvOA9Ow6G"
   },
   "source": [
    "# Part III. Applying image augmentations with torchvision\n",
    "\n",
    "Augmentations are very important in visual representation learning. \n",
    "\n",
    "In natural language processing (NLP), you don’t care about augmentations. The pretexts tasks are quite straightforward. The most common task for NLP is to predict missing words from a sentence, like BERT.\n",
    "\n",
    "Why are augmentation important in representation learning? \n",
    "\n",
    "Augmentations is an indirect way to pass human prior knowledge into the model. Typically strong augmentations are applied in pretraining backbone models with self-supervised methods. In natural images it is well-established that colour distortion and cropping are the key transformations to create augmented views. Ultimately, whatever gets transformed, don’t pay attention to it! Thus, in representation learning augmentation must maintain the image semantics (i.e. label-related information).\n",
    "\n",
    "Your task here is to understand how to make the API calls to create a transformation pipeline with multiple image augmentations.\n",
    "\n",
    "What augmentation you will apply here (**random order**):\n",
    "\n",
    "- Apply random crop of the image to a minimum of 40% the size of the image. Resize image to the initial dimension.\n",
    "- Apply 10% hue with 20% probability \n",
    "- Apply imagenet mean/std normalization\n",
    "- Apply brightness jittering, contrast jittering, saturation jittering of 30% with 80% probability \n",
    "- Apply blur with a kernel that is 20% the size of the image and sigma in [0.1 , 2] with a probability of 10%\n",
    "- Apply horizontal flip with 50% probability\n",
    "- Convert the resulting image to greyscale with 10% probability\n",
    "\n",
    "The final pipeline should consists of all the aforementioned augmentations together.\n",
    "\n",
    "`Hint`: use [torchvision](https://pytorch.org/vision/stable/)!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision.transforms as T\n",
    "import torch\n",
    "\n",
    "def Augment(img_size=32):\n",
    "### START CODE HERE ### (≈ 11 lines of code)\n",
    "    color_jitter = T.RandomApply([T.ColorJitter(0.3, 0.3, 0.3, 0)], p=0.8)\n",
    "    hue = T.RandomApply([T.ColorJitter(0, 0, 0, 0.1)], p=0.2)\n",
    "    kernel = int(img_size*0.2)\n",
    "    kernel = kernel-1 if kernel%2==0 else kernel\n",
    "    sigma = (0.1, 2.0)\n",
    "    blur =  T.RandomApply([T.GaussianBlur((kernel, kernel), sigma)], p=0.1)\n",
    "    crop = T.RandomResizedCrop(size=img_size, scale=(0.4, 1.0))\n",
    "    hflip = T.RandomHorizontalFlip(p=0.5)\n",
    "    grey = T.RandomGrayscale(p=0.1)\n",
    "    norm = T.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "    return T.Compose([\n",
    "                color_jitter,\n",
    "                hue,\n",
    "                blur,\n",
    "                crop,\n",
    "                hflip,\n",
    "                grey,\n",
    "                transforms.ToTensor(),\n",
    "                norm] )\n",
    "### END CODE HERE ###\n",
    "\n",
    "\n",
    "def test_transform(transform):\n",
    "    dl_transf = load_cifar100_data(transform=transform, train=False, batch_size=16)\n",
    "    dl = load_cifar100_data(transform=None, train=False, batch_size=16)\n",
    "    plt.figure(figsize=(8,8))\n",
    "    img_id = 15\n",
    "    plt.subplot(1,2,1)\n",
    "    imshow(next(iter(dl))[0][img_id,...])\n",
    "    plt.title('Original')\n",
    "    plt.subplot(1,2,2)\n",
    "    imshow(next(iter(dl_transf))[0][img_id,...])\n",
    "    plt.title('Transformed')\n",
    "\n",
    "transform = Augment(img_size=32)\n",
    "test_transform(transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part IV. Fine-tune the teacher model on CIFAR100\n",
    "\n",
    "- Fine-tune the teacher model for 10-20 epochs with the proposed augmentations.\n",
    "- Keep the model with the best performance on CIFAR100 validation split.\n",
    "\n",
    "`Hint`: You should be able to train the model with a batch size between 32-48 on the gcolab resources(~11.2 GB of VRAM with batch size of 48, train time per epoch with Adam ~7 minutes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 6 lines of code)\n",
    "\n",
    "# train \n",
    "#...\n",
    "# valiadte\n",
    "val_acc , val_loss_epoch = validate(teacher, val_loader, device)\n",
    "print(f\"Validation accuracy: {val_acc:.2f}%, Validation loss: {val_loss_epoch:.4f}\")\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "```\n",
    "Model teacher_cifar100_vitb.pth is loaded from epoch 14 , loss 0.8871352076530457\n",
    "Validation accuracy: 77.89%, Validation loss: 0.8871\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part V. Train ViT-tiny (student) from scratch (random init) with cross-entropy (without knowledge distilation)\n",
    "\n",
    "- Use the implemented augmentation pipeline for training the model from scratch \n",
    "- 40 epochs should be sufficient to get the expected performance\n",
    "\n",
    "`Hint`:\n",
    "\n",
    "```python\n",
    "dict_log = finetune(student, optimizer, 40, train_dl, val_dl, device)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### START CODE HERE ### (≈ 14 lines of code)\n",
    "\n",
    "# train code\n",
    "dict_log = finetune(student, optimizer, 40, train_dl, val_dl, device)\n",
    "#val code....\n",
    "val_acc, val_loss = validate(student, val_dl, device)\n",
    "\n",
    "### END CODE HERE ###\n",
    "print(\"ViT Tiny trained from random init on CIFAR100 with augmentations: Val acc\", val_acc, \"Val loss\" , val_loss )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected result\n",
    "\n",
    "```\n",
    "Model vit-tiny.pth is loaded from epoch 37 , loss 1.7089426517486572\n",
    "ViT Tiny trained from random init on CIFAR100 with augmentations: Val acc 55.76 Val loss 1.7089427\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VI. Implement knowledge distillation loss\n",
    "\n",
    "$$ Loss = (1- \\alpha) CE(p_s,y) + \\alpha KL(p_s, p_t) $$\n",
    "\n",
    "Where y is the labeled target, $p_s,p_t$ are the student/teacher prediction probabilities.\n",
    "\n",
    "KL is the KL divergence and CE is the cross entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "class CriterioDistill():\n",
    "    ### START CODE HERE ### (≈ 11 lines of code)\n",
    "    def __init__(self, alpha=0.5):\n",
    "        ...\n",
    "\n",
    "    def kl_div(self, p, q):\n",
    "        ...\n",
    "    \n",
    "    def __call__(self, logits_teacher, logits_student, labels):\n",
    "        ...\n",
    "    ### END CODE HERE ###\n",
    "\n",
    "def test_criterio_distill():\n",
    "    torch.manual_seed(42)\n",
    "    criterion = CriterioDistill()\n",
    "    logits_teacher = torch.randn(10, 100)\n",
    "    logits_student = torch.randn(10, 100)\n",
    "    labels = torch.randint(0, 100, (10,))\n",
    "    loss = criterion(logits_teacher, logits_student, labels)\n",
    "    print(loss)\n",
    "    assert loss.shape == torch.Size([]), \"Wrong shape for loss\"\n",
    "\n",
    "test_criterio_distill()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "\n",
    "\n",
    "```tensor(3.0764)```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VII. Implement MultiScaleData loading\n",
    "\n",
    "The new dataset class should return the image in the original dimension (32x32), the image rescaled to 224 , and the label.\n",
    "\n",
    "Why? Because the vit_tiny will accept images of size 32x32, while the teacher vit_base will accept images of 224x224."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiScaleData(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset, scale=224, transform=None):\n",
    "        self.dataset = dataset\n",
    "        ### START CODE HERE ### (≈ 3 lines of code) \n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    def __getitem__(self, index):\n",
    "        ### START CODE HERE ### (≈ 4 lines of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "        return img, img_rescaled, label\n",
    "    def __len__(self):\n",
    "        return len(self.dataset)\n",
    "\n",
    "def load_multiscale_data(transform=None, batch_size=64, num_workers=2, shuffle=False):\n",
    "    pin = True\n",
    "    if transform is None:\n",
    "        transform = get_transform_plain()\n",
    "    dataset = torchvision.datasets.CIFAR100(root='../data',  transform=None, train=True, download=True)\n",
    "    dataset_rescaled = MultiScaleData(dataset, scale=224, transform=transform)\n",
    "    loader = torch.utils.data.DataLoader(dataset_rescaled, \n",
    "                                         batch_size=batch_size,\n",
    "                                         shuffle=shuffle,\n",
    "                                         num_workers=num_workers,\n",
    "                                         pin_memory=pin)\n",
    "    return loader\n",
    "\n",
    "def test_MultiScaleData():\n",
    "    transform = get_transform_plain()\n",
    "    loader = load_multiscale_data(transform=transform, batch_size=4, num_workers=2, shuffle=False)\n",
    "    imgs, imgs_rescaled, labels = next(iter(loader))\n",
    "    assert imgs.shape == (4, 3, 32, 32), \"Wrong shape for img\"\n",
    "    assert imgs_rescaled.shape == (4, 3, 224, 224), \"Wrong shape for img_rescaled\"\n",
    "    print(\"Success!\")\n",
    "\n",
    "test_MultiScaleData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part VIII. Adjust training code to support knowledge distillation from the teacher model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch_distill(model, optimizer, train_loader, device, teacher, alpha=0.5):\n",
    "    model.train()\n",
    "    teacher.eval()\n",
    "    criterion_distill = CriterioDistill(alpha)\n",
    "    loss_step = []\n",
    "    correct, total = 0, 0\n",
    "    for data in train_loader:\n",
    "        ### START CODE HERE ### (≈ 14 line of code)\n",
    "        \n",
    "        ### END CODE HERE ###\n",
    "    loss_curr_epoch = np.mean(loss_step)\n",
    "    train_acc = (100 * correct / total).cpu()\n",
    "    return loss_curr_epoch, train_acc\n",
    "\n",
    "# GIVEN.\n",
    "def train_distill(model, teacher, optimizer, num_epochs, train_loader, val_loader, device, prefix='model', alpha=0.5):\n",
    "    best_val_loss = 1e8\n",
    "    best_val_acc = 0\n",
    "    model, teacher = model.to(device), teacher.to(device)\n",
    "    dict_log = {\"train_acc_epoch\":[], \"val_acc_epoch\":[], \"loss_epoch\":[], \"val_loss\":[]}\n",
    "    pbar = tqdm(range(num_epochs))\n",
    "    for epoch in pbar:\n",
    "        loss_curr_epoch, train_acc = train_one_epoch_distill(model, optimizer, train_loader, device, teacher, alpha)\n",
    "        val_acc, val_loss = validate(model, val_loader, device)\n",
    "\n",
    "        # Print epoch results to screen \n",
    "        msg = (f'Ep {epoch}/{num_epochs}: Accuracy : Train:{train_acc:.2f} \\t Val:{val_acc:.2f} || Loss: Train {loss_curr_epoch:.3f} \\t Val {val_loss:.3f}')\n",
    "        pbar.set_description(msg)\n",
    "        # Track stats\n",
    "        dict_log[\"train_acc_epoch\"].append(train_acc)\n",
    "        dict_log[\"val_acc_epoch\"].append(val_acc)\n",
    "        dict_log[\"loss_epoch\"].append(loss_curr_epoch)\n",
    "        dict_log[\"val_loss\"].append(val_loss)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': val_loss,\n",
    "                  }, f'{prefix}_best_model_min_val_loss.pth')\n",
    "        \n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                  'epoch': epoch,\n",
    "                  'model_state_dict': model.state_dict(),\n",
    "                  'optimizer_state_dict': optimizer.state_dict(),\n",
    "                  'loss': val_loss,\n",
    "                  }, f'{prefix}_best_model_max_val_acc.pth')\n",
    "    return dict_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ko1f518nTnxT"
   },
   "source": [
    "# Part IX. Train student with knowledge distillation\n",
    "\n",
    "\n",
    "- Plug everything together and show the training curves, even if you train for a small number of epochs.\n",
    "- Print the validation accuracy of the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Jvc_wMFNfWSK",
    "outputId": "2f8caadc-6d97-421c-8bae-6c2bb1f7e69c"
   },
   "outputs": [],
   "source": [
    "# Comments will be kept for help!\n",
    "### START CODE HERE ### (≈ 16 lines of code)  \n",
    "# Load teacher\n",
    "\n",
    "# Load student\n",
    "\n",
    "# Define optimizer\n",
    "\n",
    "# Define transforms and loaders\n",
    "\n",
    "\n",
    "\n",
    "# Launch training!\n",
    "dict_log = train_distill(student, teacher, optimizer, num_epochs, train_dl, val_dl, device, prefix='distill_vit_t_kl_ts', alpha=alpha)\n",
    "plt.figure(figsize=(10,10))\n",
    "plot_stats(dict_log, modelname=\"distilled_ViT-tiny_\", title=\"distilled_ViT-tiny_\")\n",
    "plt.savefig(\"ViT-tiny_CIFAR100_DISTILL___.png\")\n",
    "\n",
    "val_acc, val_loss = validate(student, val_dl, device)\n",
    "print(f\"Validation accuracy: {val_acc:.2f} \\t Validation loss: {val_loss:.3f}\")\n",
    "\n",
    "\n",
    "### END CODE HERE ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected results\n",
    "You can train the model even for 40 epochs (~120 min) and get a validarion accuracy of ~60%, already significantly better than training the vit-tiny from scratch on cifar100 without distillation.\n",
    "\n",
    "Here is our results for reference and in case you were curious to see what's the outcome of training for more epochs. Of course, we don't expect you to reproduce our results with the gcolab resources but here is what we got (for alpha=0.7 - maybe not be the optimal choice):\n",
    "\n",
    "Our best experiment for reference:\n",
    "```\n",
    "Validation accuracy: 64.88 \t Validation loss: 1.410\n",
    "```\n",
    "Remember that ViT Tiny trained from random init on CIFAR100 with augmentations: `Val acc 55.76 || Val loss 1.71`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RK4j971u_4yu"
   },
   "source": [
    "# Conclusion and Bonus reads\n",
    "\n",
    "That's the end of this exercise. If you reached this point, **congratulations**!\n",
    "\n",
    "\n",
    "Can you find a better alpha? How can alpha be interpreted? \n",
    "\n",
    "What additional losses did the the paper \"Be Your Own Teacher: Improve the Performance of Convolutional Neural Networks via Self Distillation\" intoduce?\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "[Exercise 3 solution] - Self-distillation on CIFAR100 .ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "dc5fcf396fe0abd4fa852aee332a0572494dcaf5776820055c87d9b84157f362"
   }
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "15451b89dea54127867d368514cfea78": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "4d335008de124ef1890de5087a8254f8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_15451b89dea54127867d368514cfea78",
      "placeholder": "​",
      "style": "IPY_MODEL_a1a228b8e820488aa9d7a39326832b43",
      "value": ""
     }
    },
    "5c84f8f441eb4b65a2947a8b0076b78c": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "947edc38a98549e79c0906847f20560b": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9bb9f2adc1f4480ba6ae6b390eda4521": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a0db04ae470a41c098cb9b59a67e899b": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a1a228b8e820488aa9d7a39326832b43": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "a83800177edb46bdb5789bd0507c55b1": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bb9f2adc1f4480ba6ae6b390eda4521",
      "max": 169001437,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_d6ea4f8b6c304a279b56769a81897f5f",
      "value": 169001437
     }
    },
    "aca8ccf946814899be23fad49d4c4ecb": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_a0db04ae470a41c098cb9b59a67e899b",
      "placeholder": "​",
      "style": "IPY_MODEL_947edc38a98549e79c0906847f20560b",
      "value": " 169001984/? [00:10&lt;00:00, 16869945.76it/s]"
     }
    },
    "ae807dfd8be647d29b923f286caa47c4": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_4d335008de124ef1890de5087a8254f8",
       "IPY_MODEL_a83800177edb46bdb5789bd0507c55b1",
       "IPY_MODEL_aca8ccf946814899be23fad49d4c4ecb"
      ],
      "layout": "IPY_MODEL_5c84f8f441eb4b65a2947a8b0076b78c"
     }
    },
    "d6ea4f8b6c304a279b56769a81897f5f": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
